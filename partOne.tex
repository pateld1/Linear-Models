% DO NOT COMPILE THIS TEX FILE
% COMPILE MAIN TEX FILE

\section{Linear Regression with One Predictor Variable}
\subsection{Relations between Variables}
\begin{itemize}
\item Regression analysis is a statistical methodology that utilizes the relation between two or more quantitative variables so that a response or outcome variable can be predicted from the other, or others 
\item A functional relation between two variables is expressed as follows: if $X$ denotes the independent variable and $Y$ the dependent variable, a functional relation is of the form $$Y = f(X) $$ Given a particular value of $X$, the function $f$ indicates the corresponding value of $Y$ 
\item A statistical relation, unlike a functional relation, is not a perfect one; in general, the observations for a statistical relation do not fall directly on he curve of relationship
\item Statistical relations can be highly useful, even though they do not have the exactitude of a functional relation
\end{itemize}

\subsection{Regression Models and their Uses}
\begin{itemize}
\item A regression model is a formal means of expressing the two essential ingredients of a statistical relation: 
\begin{itemize}
\item A tendency of the response variable $Y$ to vary with the predictor variable $X$ in a systematic fashion
\item A scattering of points around the curve of statistical relationship
\end{itemize}
\item These two characteristics are embodied in a regression model by postulating that: \begin{itemize}
\item There is a probability distribution of $Y$ for each level of $X$ 
\item The means of these probability distributions vary in some systematic fashion with $X$ \end{itemize} 
\item The systematic relationship between $X$ and $Y$ is called the regression function of $Y$ on $X$; the graph of the regression function is called the regression curve 
\item Regression models may differ in the form of the regression function (linear, curvilinear), in the shape of the probability distribution of $Y$ (symmetrical, skewed), and in other ways 
\item Regression models may contain more than one predictor variable 
\item Since reality must be reduced to manageable proportions whenever models are constructed, only a limited number of explanatory or predictor variables can, or should, be included in a regression model for any situation of interest 
\item A central problem in many exploratory studies is therefore that of choosing, for a regression model, a set of predictor variables that is ``good" in some sense for the purposes of the analysis
\item The choice of the functional form of the regression relation is tied to the choice of the predictor variables; sometimes, relevant theory may indicate the appropriate functional form 
\item More frequently, the functional form of the regression relation is not known in advance and must be decided upon empirically once the data have been collected
\item Linear or quadratic regression functions are often used as satisfactory first approximations to regression functions of unknown nature 
\item In formulating a regression model, the coverage is usually restricted to some interval or region of values of the predictor variable(s) which is determined either by the design of the investigation or by the range of data at hand 
\item Regression analysis serves three major purposes: description, control and prediction 
\item The existence of a statistical relation between the response variable $Y$ and the explanatory or predictor variable $X$ does not imply in any way that $Y$ depends casually on $X$ 
\end{itemize}

\subsection{Simple Linear Regression Model with Distribution of Error Terms Unspecified}
\begin{itemize}
\item A basic regression model where there is only one predictor variable and the regression function is linear can be stated as follows: $$ Y_i = \beta_0 + \beta_1X_i + \eps_i$$ where 
\begin{itemize}[label={}]
\item $Y_i$ is the value of the response variable in the $i$th trial
\item $\beta_0$ and $\beta_1$ are parameters 
\item $X_i$ is a known constant, namely, the value of the predictor variable in the $i$th trial
\item $\eps_i$ is a random error with mean $\expe{\eps_i} = 0$ and variance $\var{\eps_i} = \sigma^2$; $\eps_i$ and $\eps_j$ are uncorrelated so that their covariance is zero (i.e., $\cov{\eps_i}{\eps_j} = 0$ for all $i,j; i \neq j$) 
\item $i = 1, \dots, n$ \end{itemize} 
\item This regression model is said to be simple, linear in its parameters, and linear in the predictor variable \newpage
\item Important Features of the Model \begin{enumerate}
\item The response $Y_i$ in the $i$th trial is the sum of two components: (1) the constant term $\beta_0 + \beta_1X_i$ and (2) the random term $\eps_i$; hence $Y_i$ is a random variable 
\item Since $\expe{\eps_i} = 0$, then
$$ \expe{Y_i} = \expe{\beta_0 + \beta_1X_i + \eps_i} = \beta_0 + \beta_1X_i + \expe{\eps_i} = \beta_0 + \beta_1X_i $$ 
Thus, the response $Y_i$, when the level of $X$ in the $i$th trial is $X_i$, comes from a probability distribution whose mean is $$ \expe{Y_i} = \beta_0 + \beta_1X_i$$ 
\item The response $Y_i$ in the $i$th trial exceeds or falls short of the value of the regression function by the error term amount $\eps_i$
\item The error terms $\eps_i$ are assumed to have constant variance $\sigma^2$ and so the responses $Y_i$ have the same constant variance $$ \var{Y_i} = \sigma^2 $$ 
\end{enumerate}
\item The parameters $\beta_0$ and $\beta_1$ in the regression model are called regression coefficients
\item The parameter $\beta_1$ is the slope of the regression line (indicating the change in the mean of the probability distribution of $Y$ per unit change in $X$)
\item The parameter $\beta_0$ is the $Y$ intercept of the regression line 
\item When the scope of the model includes $X=0$, $\beta_0$ gives the mean of the probability distribution of $Y$ at $X=0$; when the scope of the model does not cover $X=0$, $\beta_0$ does not have any particular meaning as a separate term in the regression model
\item The simple linear regression model can be written equivalently as follows: let $X_0$ be a constant identically equal to $1$, then $$ Y_i = \beta_0X_0 + \beta_1X_i + \eps_i \text{ where } X_0 \equiv 1 $$ This version of the model associates an $X$ variable with each regression coefficient 
\item An alternative modification is to use for the predictor variable the deviation $X_i - \Xbar$ rather than $X_i$, then 
$$ \begin{aligned} Y_i &= \beta_0 + \beta_1(X_i - \Xbar) + \beta_1\Xbar + \eps_i \\ 
&= (\beta_0 + \beta_1\Xbar) + \beta_1(X_i - \Xbar) + \eps_i \\ 
&= \beta_0^* + \beta_1(X_9 - \Xbar) + \eps_i \end{aligned} $$ 
Thus this alternative model is $$ Y_i = \beta_0^* + \beta_1(X_i - \Xbar) + \eps_i $$ where $$\beta_0^* = \beta_0 + \beta_1\Xbar$$ 
\end{itemize}

\subsection{Data for Regression Analysis}
\begin{itemize}
\item Data for regression analysis may be obtained from nonexperimental or experimental studies
\item Observational data are data obtained from nonexperimental studies; such studies do not control he explanatory or predictor variable(s) of interest 
\item A major limitation of observational data is that they often do not provide adequate information about cause and effect relationships 
\item Frequently, it is possible to conduct a controlled experiment to provide data from which the regression parameters can be estimated 
\item When control over the explanatory variable(s) is exercised through random assignments, the resulting experimental data provide much stronger information about cause and effect relationships than do observational data; the reason is that randomization tends to balance out the effects of any other variables that might affect the response variable
\item In the terminology of experimental design, a treatment is the object being measured and the experimental units are the subjects of the study, from whom the treatment is done on and measured; control over the explanatory variable(s) then consists of assigning a treatment to each of the experimental units by means of randomization
\item The most basic type of statistical design for making randomized assignments of treatments to experimental units (or vice versa) is the completely randomized design; with this design, the assignments are made completely at random
\item This complete randomization provides that all combinations of experimental units assigned to the different treatments are equally likely, which implies that every experimental unit has an equal change to receive any one of the treatment
\item A completely randomized design is particularly useful when the experimental units are quite homogeneous; this design is very flexible; it accommodates any number of treatments and permits different sample sizes for different treatments 
\item Its chief disadvantage is that, when the experimental units are heterogeneous, this design is not as efficient as some other statistical designs
\end{itemize}

\subsection{Overview of Steps in Regression Analysis}
\begin{itemize}
\item The regression models given in the following chapters can be used either for observational data or for experimental data from a completely randomized design (regression analysis can also utilize data from other types of experimental designs, bu the regression models presented here will need to be modified) 
\item Typical Strategy for Regression Analysis \begin{enumerate} 
\item Start 
\item Exploratory data analysis
\item Develop one or more tentative regression models 
\item Is one or more of the regression models suitable for the data at hand? \begin{itemize} 
\item If yes, continue
\item If no, revise regression models and/or develop new ones and answer the question again \end{itemize}
\item Identify most suitable model
\item Make inferences on basis of regression model 
\item Stop \end{enumerate} 
\end{itemize}

\subsection{Estimation of Regression Function}
\begin{itemize}
\item The observational or experimental data to be used for estimating the parameters of the regression function consist of observations on the explanatory or predictor variable $X$ and the corresponding observations on the response variable $Y$; for each trial, there is an $X$ observation and a $Y$ observation; denote the $(X,Y)$ observations for he first trial as $(X_1,Y_1)$, for the second trial as $(X_2,Y_2)$, and in general for the $i$th trial as $(X_i, Y_i)$, where $i=1,\dots,n$
\item For the observations $(X_i, Y_i)$ for each case, the method of least squares considers the deviation of $Y_i$ from its expected value $Y_i - (\beta_0 + \beta_1X_i)$; in particular, the method of least squares requires considering the sum of the $n$ squared deviations; this criterion is denoted by $Q$: $$ Q = \sum_{i=1}^n (Y_i - \beta_0 - \beta_1X_i)^2 $$ 
According to the method of least squares, the estimators of $\beta_0$ and $\beta_1$ are those values $b_0$ and $b_1$, respectively, that minimize the criterion $Q$ for the given sample observations $(X_1,Y_1),\dots, (X_n,Y_n)$
\item The estimators $b_0$ and $b_1$ that satisfy the least squares criterion can be found in two basic ways: \begin{itemize} 
\item Numerical search procedures can be used hat evaluate in a systematic fashion the least squares criterion for different estimates $b_0$ and $b_1$ until the ones that minimize $Q$ are found
\item Analytical procedures can often be used to find the values of $b_0$ and $b_1$ that minimize $Q$; this is feasible when the regression model is not mathematically complex \end{itemize}
\item Using the analytical approach, the values $b_0$ and $b_1$ that minimize $Q$ for the simple linear regression model  are given by the following simultaneous equations: $$ \begin{aligned} 
\sum Y_i &= nb_0 + b_1\sum X_i \\ \sum X_i Y_i &= b_0\sum X_i + b_1\sum X_i^2 \end{aligned} $$ 
These two equations are called normal equations; $b_0$ and $b_1$ are called point estimators of $\beta_0$ and $\beta_1$ respectively
\item The normal equations can be solved simultaneously for $b_0$ and $b_1$: $$ \begin{aligned} 
b_1 &= \frac{ \sum (X_i - \Xbar)(Y_i - \Ybar)}{\sum (X_i - \Xbar)^2} \\
b_0 &= \frac{1}{n} \left( \sum Y_i - b_1 \sum X_i \right) = \Ybar - b_1\Xbar \end{aligned} $$ where $\Xbar$ and $\Ybar$ are the means of the $X_i$ and $Y_i$ observations respectively
\item Derivation of above result: for given sample observations $(X_i, Y_i)$, the quantity $Q$ is a function of $\beta_0$ and $\beta_1$;  the values of $\beta_0$ and $\beta_1$ that minimize $Q$ can be derived by differentiating $Q$ with respect to $\beta_0$ and $\beta_1$: $$ \begin{aligned} \frac{\partial Q}{\partial \beta_0} &= -2\sum (Y_i - \beta_0 - \beta_1X_i) \\ \frac{\partial Q}{\partial \beta_1} &= -2\sum X_i(Y_i - \beta_0 - \beta_1X_i) \end{aligned} $$ 
Setting these partial derivatives to zero, using $b_0$ and $b_1$ to denote the particular values of $\beta_0$ and $\beta_1$ that minimize Q and simplifying, the following is obtained $$ \begin{aligned} \sum_{i=1}^n (Y_i - b_0 - b_1X_i) &= 0 \\ \sum_{i=1}^n X_i(Y_i - b_0 - b_1X_i) &= 0 \end{aligned} $$ Expanding this, the following is true: $$ \begin{aligned} \sum Y_i - nb_0 - b_1\sum X_i &= 0 \\ \sum X_iY_i - b_0\sum X_i - b_2\sum X_i^2 &= 0 \end{aligned} $$ 
By rearranging terms, the normal equations are obtained
\item Gauss-Markov Theorem: Under the conditions of the simple linear regression model the least squares estimators $b_0$ and $b_1$, as given above, are unbiased and have minimum variance among all unbiased linear estimators
\item This theorem states first that $b_0$ and $b_1$ are unbiased estimators and so $$ \expe{b_0} = \beta_0 ~~~ \expe{b_1} = \beta_1 $$ so that neither estimator tends to overestimate or underestimate systematically 
\item Second, the theorem states that the estimators $b_0$ and $b_1$ are more precise (o.e., their sampling distributions are less variable) than any other estimators belonging to the class of unbiased estimators that are linear functions of the observations $Y_1,\dots,Y_n$; the estimators $b_0$ and $b_1$ are such linear functions of the $Y_i$
\item Given sample estimators $b_0$ and $b_1$ of the parameters in the regression function, $\expe{Y} = \beta_0 + \beta_1X$, the regression function is estimates as $$ \hat{Y} = b_0 + b_1X $$ where $\hat{Y}$ is the value of the estimated regression function at the level $X$ of the predictor variable 
\item A value of the response variable is called a response while $\expe{Y}$ is called the mean response; thus, the mean response stands for the mean of the probability distribution of $Y$ corresponding to the level $X$ of the predictor variable 
\item $\hat{Y}$ is a point estimator of the mean response when the level of the predictor variable is $X$ 
\item As an extension of the Gauss-Markov Theorem, $\hat{Y}$ is an unbiased estimator of $\expe{Y}$, with minimum variance in the class of unbiased linear estimators 
\item Let $\hat{Y}_i$ be the fitted value for the $i$th case $$ \hat{Y}_i = b_0 + b_1X_i ~~ i = 1,\dots,n$$ Thus the fitted value $\hat{Y}_i$ is to viewed in distinction to the observed value $Y_i$
\item The $i$th residual is the difference between the observed value $Y_i$ and the corresponding fitted value $\hat{Y}_i$; this residual is denoted by $e_i$ and is defined as follows: $$ e_i = Y_i - \hat{Y}_i $$ 
\item For the simple linear regression model, the residual $e_i$ becomes $$ e_i = Y_i - (b_0 + b_1X_i) = Y_i - b_0 - b_1X_i $$ 
\item The model error term value $\eps_i = Y_i - \expe{Y_i}$ involves the vertical deviation of $Y_i$ from the unknown true regression line hence is unknown; the residual $e_i = Y_i - \hat{Y}_i$ is the vertical deviation of $Y_i$ from the fitted value $\hat{Y}_i$ on the estimated regression line, and it is known
\item Properties of Fitted Regression Line \begin{enumerate} 
\item The sum of the residuals is zero $$ \sum_{i=1}^n e_i = 0 $$ 
\item The sum of the squared residuals, $\sum e_i^2$ is a minimum 
\item The sum of the observed values $Y_i$ equals the sum of the fitted values $\hat{Y}_i$: $$ \sum_{i=1}^n Y_i = \sum_{i=1}^n \hat{Y}_i $$ 
\item The sum of the weighted residuals is zero when he residual in the $i$th trial is weighted by the level of the predictor variable in the $i$th trial: $$ \sum_{i=1}^n X_ie_i = 0 $$ 
\item The sum of the weighted residuals is zero when the residual in the $i$th trial is weighted by the fitted value of the response variable for the $i$th trial: $$ \sum_{i=1}^n \hat{Y}_ie_i = 0 $$ 
\item The regression line always goes through the point $(\Xbar, \Ybar)$ \end{enumerate} 
\end{itemize}

\subsection{Estimation of Error Terms Variance $\sigma^2$}
\begin{itemize}
\item The variance $\sigma^2$ of the error terms $\eps_i$ in the regression model needs to be estimated to obtain an indication of the variability of the probability distribution of $Y$
\item The variance $\sigma^2$ of a single population is estimated by the sample variance $s^2$ as follows: $$ s^2 = \frac{\sum_{i=1}^n (Y_i - \Ybar)^2}{n-1} $$ where the sum is called a sum of squares and $n-1$ is the degrees of freedom; this number is $n-1$ because one degree of freedom is lost by using $\Ybar$ as an estimate of the unknown population mean $\mu$ 
\item This estimator is the usual sample variance which is an unbiased estimator of the variance $\sigma^2$ of an infinite population
\item The sample variance is often called a mean square because a sum of squares has been divided by the appropriate number of degrees of freedom
\item For the regression model, the variance for each observation $Y_i$ is $\sigma^2$, the same as that of each error term $\eps_i$; a sum of squared deviations are needed to be calculated but note that the $Y_i$ now come from different probability distributions with different means that depend upon the level $X_i$; thus, the deviation of an observation $Y_i$ must be calculated around its own estimated mean $\hat{Y}_i$ 
\item The deviations are the residuals $$ Y_i - \hat{Y}_i = e_i $$ and the appropriate sum of squares, denoted by SSE, is $$ \text{SSE} = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 = \sum_{i=1}^n e_i^2 $$ where SSE stands for error sum of squares or residual sum of squares 
\item The SSE has $n-2$ degrees of freedom because both $\beta_0$ and $\beta_1$ had to be estimated in obtaining the estimated means $\hat{Y}_i$ 
\item The appropriate mean square, denoted by MSE or $s^2$ is $$ s^2 = \text{MSE} = \frac{\text{SSE}}{n-2} = \frac{\sum (Y_i - \hat{Y}_i)^2}{n-2} = \frac{\sum e_i^2}{n-2} $$ where MSE stands for error mean square or residual mean square
\item The MSE is an unbiased estimator for $\sigma^2$ for a regression model $$ \expe{\text{MSE}} = \sigma^2 $$ 
\item An estimator of the standard deviation $\sigma$ is simply $s=\sqrt{\text{MSE}}$, the positive square root of the MSE 
\end{itemize}

\subsection{Normal Error Regression Model}
\begin{itemize}
\item The normal error regression model is as follows: $$ Y_i = \beta_0 + \beta_1X_i + \eps_i $$ where \begin{itemize}[label={}]
\item $Y_i$ is the observed response in the $i$th trial
\item $X_i$ is a known constant, the level of the predictor variable in the $i$th trial
\item $\beta_0$ and $\beta_1$ are parameters
\item $\eps_i$ are independent $N(0, \sigma^2)$
\item $i=1,\dots,n$ \end{itemize} 
\item The symbol $N(0, \sigma^2)$ stands for normally distributed with mean $0$ and variance $\sigma^2$
\item The normal error model is the same as the regression model with unspecified error distribution, except this one assumes that the errors $\eps_i$ are normally distributed 
\item Since the errors are normally distributed, the assumption of uncorrelatedness of the $\eps_i$ in the regression model becomes one of independence in the normal error model
\item This model implies that the $Y_i$ are independent normal random variables, with mean $\expe{Y_i} = \beta_0 + \beta_1X_i$ and variance $\sigma^2$ 
\item The normality assumption for the error term is justifiable in many situations because the error terms frequently represent the effects of factors omitted from the model that affect the response to some extent and that vary at random without reference to the variable $X$ 
\item A second reason why the normality assumption of the error terms is frequently justifiable is that the estimation and testing procedures are based on the $t$ distribution and are usually only sensitive to large departures from normality 
\item When the functional form of the probability distribution of the error terms is specified, estimators of the parameters $\beta_0$, $\beta_1$ and $\sigma^2$ can be obtained using the method of maximum likelihood; this method chooses as estimates those values of the parameters that are most consistent with the sample data
\item The method of maximum likelihood uses the product of the densities as the measure of consistency of the parameter value with the sample data; the product is called the likelihood value of the parameter value and is denoted by $L(\cdot)$ where $\cdot$ is the parameter being estimated; if the value of $\cdot$ is consistent with the sample data, the densities will be relatively large and so will be the likelihood value; if the value of $\cdot$ is not consistent with the data, the densities will be small and the product $L(\cdot)$ will be small
\item The method of maximum likelihood chooses as the maximum likelihood estimate that value of $\cdot$ for which the likelihood value is largest; there are two methods of finding the estimates: by a systematic numerical search or by use of an analytical solution 
\item The product of the densities viewed as a function of the unknown parameters is called the likelihood function 
\item In general, the density of an observation $Y_i$ for the normal error regression model is as follows, utilizing the fact that $\expe{Y_i} = \beta_0 + \beta_1X_i$ and $\var{Y_i} = \sigma^2$: $$ f_i = \frac{1}{\sqrt{2\pi}\sigma} \exp\left[ -\frac{1}{2}\left(\frac{Y_i - \beta_0 - \beta_1X_i}{\sigma}\right)^2\right] $$ 
\item The likelihood function for $n$ observations $Y_1,\dots,Y_n$ is the product of the individual densities; since the variance $\sigma^2$ of the error terms is usually unknown, the likelihood function is a function of three parameters $\beta_0$, $\beta_1$ and $\sigma^2$ $$ \begin{aligned} L(\beta_0, \beta_1, \sigma^2) &= \prod_{i=1}^n \frac{1}{(2\pi \sigma^2)^{1/2}} \exp\left[-\frac{1}{2\sigma^2}(Y_i - \beta_0 - \beta_1X_i)^2\right] \\ &= \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left[-\frac{1}{2\sigma^2}\sum_{i=1}^n (Y_i - \beta_0 - \beta_1X_i)^2\right] \end{aligned} $$ 
\item The values of $\beta_0$, $\beta_1$ and $\sigma^2$ that maximize this likelihood function are the maximum likelihood estimators and are denoted by $\hat{\beta}_0$, $\hat{\beta}_1$ and $\hat{\sigma}^2$ respectively; these estimators are calculated analytically and are as follows: $$ \begin{tabular}{c|c} \hline 
Parameter & Maximum Likelihood Estimator \\ \hline 
$\beta_0$ & $\hat{\beta}_0 = b_0 = \frac{1}{n}\left(\sum Y_i - b_1\sum X_i\right) = \Ybar - b_1\Xbar $ \\ \hline
$\beta_1$ & $\hat{\beta}_1 = b_1 = \frac{\sum (X_i - \Xbar)(Y_i - \Ybar)}{\sum (X_i - \Xbar)^2} $ \\ \hline 
$\sigma^2$ & $\hat{\sigma}^2 = \frac{\sum (Y_i - \hat{Y}_i)^2}{n} $ \\ \hline \end{tabular} $$ 
\item Thus, the maximum likelihood estimators of $\beta_0$ and $\beta_1$ are the same estimators as those provided by the methods of least squares; the maximum likelihood estimator $\hat{\sigma}^2$ is biased and ordinarily the unbiased MSE or $s^2$ is used 
\item The unbiased MSE or $s^2$ differs but slightly from the maximum likelihood estimator $\hat{\sigma}^2$, especially if $n$ is not small $$ s^2 = \text{MSE} = \frac{n}{n-2}\hat{\sigma}^2 $$ 
\item Since the maximum likelihood estimators of $\hat{\beta}_0$ and $\hat{\beta}_1$ are the same as the least squares estimators $b_0$ and $b_1$, they have the properties of all least squares estimators: \begin{itemize}
\item There are unbiased 
\item They have minimum variance among all unbiased linear estimators \end{itemize}
\item In addition, the maximum likelihood estimators $b_0$ and $b_1$ for the normal error regression model have other desirable properties: \begin{itemize}
\item They are consistent
\item They are sufficient 
\item They are minimum variance unbiased; that is, they have minimum variance in the class of all unbiased estimators (linear or otherwise) 
\end{itemize} 
\item Derivation of maximum likelihood estimators: take partial derivatives of $L$ with respect to $\beta_0$, $\beta_1$ and $\sigma^2$, equating each of the partials to zero and solving the system of equations obtained; work with $\log_e L$ rather than $L$ since both are maximized for the same values of $\beta_0$, $\beta_1$ and $\sigma^2$: $$ \log L = -\frac{n}{2}\log 2\pi - \frac{n}{2} \log \sigma^2 - \frac{1}{2\sigma^2}\sum (Y_i - \beta_0 - \beta_1X_i)^2 $$ 
The partial derivatives are as shown: $$ \begin{aligned} \frac{\partial \log L}{\partial \beta_0} &= \frac{1}{\sigma^2} \sum (Y_i - \beta_0 - \beta_1X_i) \\ \frac{\partial \log L}{\partial \beta_1} &= \frac{1}{\sigma^2} \sum X_i(Y_i - \beta_0 - \beta_1X_i) \\ \frac{\partial \log L}{\partial \sigma^2} &= -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}\sum (Y_i - \beta_0 - \beta_1X_i)^2 \end{aligned} $$  Setting these partial derivatives equal to zero and replacing $\beta_0$, $\beta_1$ and $\sigma^2$ by the estimators $\hat{\beta}_0$, $\hat{\beta}_1$ and $\hat{\sigma}^2$, and after some simplifications: $$ \begin{aligned} \sum (Y_i - \hat{\beta}_0 - \hat{\beta}_1X_i) &= 0 \\ \sum X_i(Y_i - \hat{\beta}_0 - \hat{\beta}_1X_i) &= 0 \\ \frac{\sum (Y_i - \hat{\beta}_0 - \hat{\beta}_1X_i)^2}{n} &= \hat{\sigma}^2 \end{aligned} $$ 
The first two equations are identical to the earlier least squares normal equations and the last one is the biased estimator of $\sigma^2$ as given earlier
\end{itemize}


\section{Inferences in Regression and Correlation Analysis}
Throughout this chapter (excluding Section 2.11), and in the remainder of Part I unless otherwise stated, assume that the normal error regression model is applicable. This model is
$$ Y_i = \beta_0 + \beta_1X_i + \eps_i $$ where: \begin{itemize}[label={}]
\item $\beta_0$ and $\beta_1$ are parameters 
\item $X_i$ are known constants 
\item $\eps_i$ are independent $N(0, \sigma^2)$ \end{itemize} 

\subsection{Inferences Concerning $\beta_1$}
\begin{itemize}
\item At times, tests concerning $\beta_1$ are of interest, particularly one of the form: \hyptest{\beta_1 = 0}{\beta_1 \neq 0} 
\item The reason for interest in testing whether or not $\beta_1 = 0$ is that, when $\beta_1 = 0$, there is no linear association between $Y$ and $X$
\item When $\beta_1 = 0$, the regression line is horizontal and the means of the probability distributions of $Y$ are therefore all equal, namely: $$\expe{Y} = \beta_0 + (0)X = \beta_0 $$ 
\item $\beta_1 = 0$ for the normal error regression model also implies that there is no relation of any type between $Y$ and $X$ since the probability distributions of $Y$ are then identical at all levels of $X$
\item The point estimator $b_1$ is as follows: $$ b_1= \frac{\sum (X_i - \Xbar)(Y_i- \Ybar)}{\sum (X_i - \Xbar)^2} $$ 
\item The sampling distribution of $b_1$ refers to the different values of $b_1$ that would be obtained with repeated sampling when the levels of the predictor variable $X$ are held constant from sample to sample 
\item For the normal error regression model, the sampling distribution of $b_1$ is normal, with mean and variance: $\expe{b_1} = \beta_1$ and $\var{b_1} = \frac{\sigma^2}{\sum (X_i - \Xbar)^2} $
\item $b_1$ can be expressed as follows: $$ b_1 = \frac{\sum (X_i - \Xbar)(Y_i - \Ybar)}{\sum (X_i - \Xbar)^2} = \sum k_iY_i $$ where $k_i = \frac{X_i - \Xbar}{\sum (X_i - \Xbar)^2} $; Observe that the $k_i$ are a function of the $X_i$ are therefore are fixed quantities since the $X_i$ are fixed; hence, $b_1$ is a linear combination of the $Y_i$ where the coefficients are solely a function of the fixed $X_i$
\item The coefficients $k_i$ have a number of interesting properties 
$$ \sum k_i = 0 ~~~~~~~ \sum k_iX_i = 1 ~~~~~~~ \sum k_i^2 = \frac{1}{\sum (X_i - \Xbar)^2} $$ 
\item To show that $b_1$ is a linear combination of the $Y_i$ with coefficients $k_i$, first prove that $$ \sum (X_i - \Xbar)(Y_i - \Ybar) = \sum (X_i - \Xbar)Y_i $$ This follows since $$ \sum (X_i - \Xbar)(Y_i - \Ybar) = \sum (X_i - \Xbar)Y_i - \sum (X_i - \Xbar)\Ybar $$ But $\sum (X_i. - \Xbar)\Ybar = \Ybar \sum (X_i - \Xbar) = 0$ since $\sum (X_i - \Xbar) = 0$. Now 
$$ b_1 = \frac{\sum (X_i - \Xbar)(Y_i - \Ybar)}{\sum (X_i - \Xbar)^2} = \frac{\sum (X_i - \Xbar)Y_i}{\sum (X_i - \Xbar)^2} = \sum k_i Y_i $$ 
\item The normality of the sampling distribution of $b_1$ follows at once from the fact that $b_1$ is a linear combination of the $Y_i$; the $Y_i$ are independently, normally distributed; note that a linear combination of independent normal random variables is normally distributed 
\item The unbiasedness of the point estimator $b_1$, stated in the Gauss-Markov theorem, can be proved as follows: 
$$ \begin{aligned} \expe{b_1} &= \expe{\sum k_iY_i} = \sum k_i \expe{Y_i} = \sum k_i(\beta_0 + \beta_1X_i) \\ &= \beta_0 \sum k_i + \beta_1 \sum k_iX_i \end{aligned} $$ and so $\expe{b_1} = \beta_1$
\item The variance of $b_1$ can be derived as follows: $$ \begin{aligned} 
\var{b_1} &= \var{\sum k_iY_i} = \sum k_i^2 \var{Y_i} \\ &= \sum k_i^2 \sigma^2 = \sigma^2 \sum k_i^2 \\ &= \frac{\sigma^2}{\sum (X_i - \Xbar)^2} \end{aligned} $$ 
\item The variance of the sampling distribution of $b_1$ can be estimated by replacing $\sigma^2$ with MSE, the unbiased estimator of $\sigma^2$
$$ s^2[b_1] = \frac{\text{MSE}}{\sum (X_i - \Xbar)^2} $$ which is an unbiased estimator of $\var{b_1}$
\item Since $b_1$ is normally distributed, the standardized statistic $\frac{b_1 - \beta_1}{\sig{b_1}}$ is a standard normal variable
\item When a statistic is standardized but the denominator is an estimated standard deviation rather than the true standard deviation, it is called a studentized statistic
\item Theorem: $$ \frac{b_1 - \beta_1}{\sd{b_1}} \text{ is distributed as } t_{n-2} \text{ for the normal error regression model} $$ 
\item Proof: Note that $\frac{\text{SSE}}{\sigma^2}$ is distributed as $\chi^2$ with $n-2$ degrees of freedom and is independent of $b_0$ and $b_1$. First rewrite $(b_1 - \beta_1)/\sd{b_1}$ as follows:
$$ \frac{b_1-\beta_1}{\sig{b_1}} / \frac{\sd{b_1}}{\sig{b_1}} $$ 
The numerator is a standard normal variable $z$; now, $$ \begin{aligned} \frac{s^2[b_1]}{\sigma^2[b_1]} &= \frac{ \frac{\text{MSE}}{\sum (X_i - \Xbar)^2}}{ \frac{\sigma^2}{\sum (X_i - \Xbar)^2}} = \frac{\text{MSE}}{\sigma^2} = \frac{\frac{\text{SSE}}{n-2}}{\sigma^2} \\ &= \frac{ \text{SSE}}{\sigma^2(n-2)} \sim \frac{\chidist{n-2}}{n-2} \end{aligned} $$  where the symbol $\sim$ stands for ``is distributed as"; hence
$$ \frac{b_1 - \beta_1}{s[b_1]} \sim \frac{z}{\sqrt{ \frac{\chidist{n-2}}{n-2}}} $$ But $z$ and $\chi^2$ are independent since $z$ is a function of $b_1$ and $b_1$ is independent of $\text{SSE}/\sigma^2 \sim \chi^2$ and so $$ \frac{b_1 - \beta_1}{s[b_1] \sim t_{n-2}} $$ 
\item  Since $(b_1 - \beta_1)/s[b_1]$ follows a $t$ distribution, the following can be said $$ \prob{\tdist{\frac{\alpha}{2}}{n-2} \leq \frac{b_1 - \beta_1}{s[b_1]} \leq \tdist{1 - \frac{\alpha}{2}}{n-2}} = 1 - \alpha $$ 
$\tdist{\frac{\alpha}{2}}{n-2}$ denotes the $(\alpha/2)100$ percentile of the $t$ distribution with $n-2$ degrees of freedom
\item The $1-\alpha$ confidence limits for $\beta_1$ are $$ b_1 \pm t_{1 - \frac{\alpha}{2}, n-2} s[b_1] $$ 
\item This is derived from the following: because of the symmetry of the $t$ distribution around its mean $0$, it follows that $$ \tdist{\frac{\alpha}{2}}{n-2} = -\tdist{1 - \frac{\alpha}{2}}{n-2} $$ and by rearranging the probability statement,
$$ \prob{b_1 - \tdist{1 - \frac{\alpha}{2}}{n-2}s[b_1] \leq \beta_1 \leq b_1 + \tdist{1 - \frac{\alpha}{2}}{n-2} s[b_1]} = 1 - \alpha $$ 
\item Two-Sided $T$ Test: Let the null and alternative hypotheses be \hyptest{\beta_1 = 0}{\beta_1 \neq 0} An explicit test of the alternatives is based on the test statistic $$ t^* = \frac{b_1}{s[b_1]} $$ The decision rule with this test statistic for controlling the level of significance at $\alpha$ is: \begin{itemize} 
\item If $\abs{t^*} \leq \tdist{1 - \frac{\alpha}{2}}{n-2}$, conclude $H_0$ (fail to reject $H_0$)
\item If $\abs{t^*} > \tdist{1 - \frac{\alpha}{2}}{n-2}$, conclude $H_A$ (reject $H_0$) \end{itemize} 
\item When the test of whether or not $\beta_1 = 0$ leads to the conclusion that $\beta_1 \neq 0$, the association between $Y$ and $X$ is sometimes described to be a linear statistical association 
\item The two-sided $P$-value is obtained by first finding the one-sided $P$-value and then multiplying by $2$; if it is less than $\alpha$, then conclude $H_A$ (or reject $H_0$) else conclude $H_0$ (or fail to reject $H_0$)
\item One-Sided $T$ Test: Let the null and alternative hypotheses be: \hyptest{\beta_1 \leq 0}{\beta_1 > 0} The decision rule based on this test statistic would be: \begin{itemize} 
\item If $t^* \leq \tdist{1 - \alpha}{n-2}$, conclude $H_0$ (fail to reject $H_0$)
\item If $t^* > \tdist{1-\alpha}{n-2}$, conclude $H_A$ (reject $H_0$) \end{itemize}
\item Occasionally, it is desired to test whether or not $\beta_1$ equals some specified nonzero value $v$; the alternatives now are \hyptest{\beta_1 = v}{\beta_1 \neq v} and the appropriate test statistic is $$ t^* = \frac{b_1 - v}{s[b_1]} $$ The decision rule remains the same
\end{itemize} 
\subsection{Inferences Concerning $\beta_0$}
\begin{itemize}
\item Inferences concerning $\beta_0$ only occur when the scope of the model includes $X=0$
\item The point estimator $b_0$ is as follows: $$ b_0 = \Ybar - b_1\Xbar $$ 
\item The sampling distribution of $b_0$ refers to the different values of $b_0$ that would be obtained with repeated sampling when the levels of the predictor variable $X$ are held constant from sample to sample 
\item For the normal error regression model, the sampling distribution of $b_0$ is normal with mean and variance $$ \expe{b_0} = \beta_0 ~~~~~~ \var{b_0} = \sigma^2\left[ \frac{1}{n} + \frac{\Xbar^2}{\sum (X_i - \Xbar)^2} \right] $$ 
\item The normality of the sampling distribution of $b_0$ follows because $b_0$ is a linear combination of the observations $Y_i$ and the mean and variance of the sampling distribution of $b_0$ can be derived as before for $b_1$
\item An estimator of $\var{b_0}$ is obtained by replacing $\sigma^2$ by its point estimator MSE $$ s^2[b_0] = \text{MSE}\left[ \frac{1}{n} + \frac{\Xbar^2}{\sum (X_i - \Xbar)^2} \right] $$ The positive square root, $s[b_0]$ is an estimator of $\sig{b_0}$
\item Theorem: $$ \frac{b_) - \beta_0}{s[b_0]} \text{ is distributed as } t_{n-2} \text{ for the normal error regression model} $$ 
\item The $1-\alpha$ confidence limits for $\beta_0$ are obtained in the same manner as those for $\beta_1$ and are: 
$$ b_0 \pm \tdist{1 - \frac{\alpha}{2}}{n-2} s[b_0] $$ 
\end{itemize}

\subsection{Some Considerations on Making Inferences Concerning $\beta_0$ and $\beta_1$}
\begin{itemize}
\item If the probability distribution of $Y$ are not exactly normal but do not depart seriously, the sampling distributions of $b_0$ and $b_1$ will be approximately normal and the use of the $t$ distribution will provide approximately the specified confidence coefficient or level of significance 
\item Even if the distribution of $Y$ are far from normal, the estimators $b_0$ and $b_1$ generally have the property of asymptotically normality - their distributions approach normality under very general conditions as the sample size increases
\item For large samples, the $t$ value is replaced by the $z$ value for the standard normal distribution
\item Since the regression model assumes that the $X_i$ are known constants, the confidence coefficients and risks of errors are interpreted with respect to taking repeated samples in which the $X$ observations are kept at the same levels as in the observed sample; for example, concerning a confidence interval for $\beta_1$, the coefficient is interpreted to mean that if many independent samples are taken where the levels of $X$ are the same as in the data set and a $\alpha$ confidence interval is constructed for each sample, $\alpha$ percent of the intervals will contain the true value of $\beta_1$
\item Variances of $b_1$ and $b_0$ are affected by the spacing of the $X$ levels in the observed data, as indicated by the use of $n$ and $\sigma^2$ in the formulas; for example, the greater is the spread in the $X$ levels, the larger is the quantity $\sum (X_i - \Xbar)^2$ and the smaller is the variance of $b_1$
\item The power of tests on $\beta_0$ and $\beta_1$ is the probability that the test correctly rejects the null hypothesis (concluding $H_A$)
\item For example, using the hypothesis test concerning $\beta_1$ where \hyptest{\beta_1 = v}{\beta_1 \neq v} the test statistic computed is $$ t^* = \frac{b_1 - v}{s[b_1]} $$ and the decision rule for level of significance $\alpha$ is \begin{itemize} 
\item If $\abs{t^*} \leq \tdist{1 - \frac{\alpha}{2}}{n-2}$, conclude $H_0$ (fail to reject $H_0$)
\item If $\abs{t^*} > \tdist{1 - \frac{\alpha}{2}}{n-2}$, conclude $H_A$ (reject $H_0$) \end{itemize} 
The power of test is the probability that the decision rule will lead to conclusion $H_A$ when $H_A$ in fact holds; specifically, the power of the test is given by 
$$ \text{Power} = \prob{\abs{t^*} > \tdist{1 - \frac{\alpha}{2}}{n-2} | \delta} $$ where $\delta$ is the noncentrality measure, i.e., a measure of how far the true value of $\beta_1$ is from a given value $v$ $$ \delta = \frac{\abs{\beta_1 - v}}{\sig{b_1}} $$ 
\end{itemize}

\subsection{Interval Estimation of $\expe{Y_h}$}
\begin{itemize}
\item Let $X_h$ denote the level of $X$ for which the mean response is to be estimated; it may be a value which occurred in the sample or it may be some other value of the predictor variable within the scope of the model; the mean response when $X=X_h$ is denoted by $\expe{Y_h}$ 
\item The point estimator $\hat{Y}_h$ of $\expe{Y_h}$ is $\hat{Y}_h = b_0 + b_1X_h$
\item The sampling distribution of $\hat{Y}_h$ refers to the different values of $\hat{Y}_h$ that would be obtained if repeated samples were selected, each holding the levels of the predictor variable $X$ constant, and calculating $\hat{Y}_h$ for each sample
\item For the normal error regression model, the sampling distribution of $\hat{Y}_h$ is normal with mean and variance $$ \expe{\hat{Y}_h} = \expe{Y_h} ~~~~~~ \var{\hat{Y}_h} = \sigma^2\left[ \frac{1}{n} + \frac{(X_h - \Xbar)^2}{\sum (X_i - \Xbar)^2}\right] $$ 
\item The normality of the sampling distribution of $\hat{Y}_h$ follows directly from the fact that $\hat{Y}_h$ is a linear combination of the observations $Y_i$
\item $\hat{Y}_h$ is an unbiased estimator of $\expe{Y_h}$ $$ \expe{\hat{Y}_h} = \expe{b_0 + b_1X_h} = \expe{b_0} + X_h\expe{b_1} = \beta_0 + \beta_1X_h $$ 
\item The variability of the sampling distribution of $\hat{Y}_h$ is affected by how far $X_h$ is from $\Xbar$, through the term $(X_h - \Xbar)^2$; the further from $\Xbar$ is $X_h$, the greater the quantity $(X_h - \Xbar)^2$ and the larger is the variance of $\hat{Y}_h$
\item The estimated variance of $\hat{Y}_h$ is $$ s^2[\hat{Y}_h] = \text{MSE}\left[ \frac{1}{n} + \frac{(X_h - \Xbar)^2}{\sum (X_i - \Xbar)^2} \right] $$ The estimated standard deviation of $\hat{Y}_h$ is then $s[\hat{Y}_h]$, the positive square root of $s^2[\hat{Y}_h]$
\item When $X_h = 0$, the variance of $\hat{Y}_h$ is reduced to the variance of $b_0$ since $\hat{Y}_h = b_0 + b_1X_h = b_0 + b_1(0) = b_0$
\item To derive $\sig{\hat{Y}_h}$, first show that $b_1$ and $\Ybar$ are uncorrelated and hence, for the regression model, independent: $\cov{\Ybar}{b_1} = 0$, where the LHS denotes the covariance between the two; now, $$ \Ybar = \sum \left( \frac{1}{n} \right)Y_i ~~~~~~ b_1 = \sum k_iY_i $$ where $k_i$ is defined as before; now, knowing that $Y_i$ are independent random variables, 
$$ \cov{\Ybar}{b_1} = \sum \left( \frac{1}{n} \right) k_i \sigma^2[Y_i] = \frac{\sigma^2}{n} \sum k_i $$ but $\sum k_i = 0$ and so the covariance is $0$; to find the variance of $\hat{Y}_h$, use the alternative form of the estimator $$ \var{\hat{Y}_h} = \var{\Ybar - b_1(X_h - \Xbar)} $$ Since $\Ybar$ and $b_1$ are independent and $X_h$ and $\Xbar$ are constants, then $$ \var{\hat{Y}_h} = \var{\Ybar} + (X_h - \Xbar)^2\var{b_1} $$ Since $$ \var{\Ybar}. = \frac{\var{Y_i}}{n} = \frac{\sigma^2}{n} $$ and so $$ \var{\hat{Y}_h} = \frac{\sigma^2}{n} + (X_h - \Xbar)^2\frac{\sigma^2}{\sum (X_i  - \Xbar)^2} $$ 
\item Theorem: $$ \frac{\hat{Y}_h - \expe{Y_h}}{s[\hat{Y}_h]} \text{ is distributed as } t_{n-2} \text{ for the regression model} $$ All inferences concerning $\expe{Y_h}$ are carried out in the usual fashion with the $t$ distribution
\item A confidence interval for $\expe{Y_h}$ is constructed in the standard fashion as follows $$ \hat{Y}_h \pm \tdist{1 - \frac{\alpha}{2}}{n-2}s[\hat{Y}_h] $$ 
\item Since the $X_i$ are known constants in the regression model, the interpretation of confidence intervals and risks of errors in inferences on the mean response is in terms of taking repeated samples in which the $X$ observations are at the same levels as in the actual study
\item For given sample results, the variance of $\hat{Y}_h$ is smallest when $X_h = \Xbar$; thus, in an experiment to estimate the mean response at a particular level $X_h$ of the predictor variable, the precision of the estimate will be greatest if (everything else remaining equal) the observations on $X$ are spaced so that $\Xbar = X_h$
\item The usual relationship between confidence intervals and tests applies in inferences concerning the mean response; thus, the two-sided confidence limits can be utilized for two-sided tests concerning the mean response at $X_h$; alternatively, a regular decision rule can be set up
\item The confidence limits for a mean response $\expe{Y_h}$ are not sensitive to moderate departures from the assumption that the error terms are normally distributed 
\item Confidence limits apply when a single mean response is to be estimated from the study
\end{itemize}

\subsection{Prediction of New Observation}
\begin{itemize}
\item A new observation on $Y$ to be predicted is viewed as a result of a new trial, independent of the trials on which the regression analysis is based; denote the level of $X$ for the new trial as $X_h$ and the new observation on $Y$ as $Y_{h(\text{new})}$
\item In the estimation of the mean response $\expe{Y_h}$, the mean of the distribution of $Y$ is estimated; in the prediction of a new response $Y_{h(\text{new})}$, an individual outcome drawn from the distribution of $Y$ is predicted 
\item The basic idea of a prediction interval is to choose a range in the distribution of $Y$ wherein most of the observations will fall and then to declare that the next observation will fall in this range; the usefulness of the prediction interval depends on the width of the interval and the needs for precision by the user
\item Assume that all regression parameters of the normal error regression model are known, then the $1-\alpha$ prediction limits for $Y_{h(\text{new})}$ are $$ \expe{Y_h} \pm \zdist{1-\frac{\alpha}{2}} \sigma $$ In centering the limits around $\expe{Y_h}$, the narrowest interval consistent with the specified probability of a correct prediction is obtained 
\item When the regression parameters are unknown, the mean of the distribution of $Y$ is estimated by $\hat{Y}_h$ as usual and the variance of the distribution of $Y$ is estimated by the MSE but the prediction limit above with the parameters replaced by the corresponding point estimators cannot be used since the mean $\expe{Y_h}$ itself is estimated by a confidence interval, making the location of the distribution of $Y$ uncertain
\item Prediction limits for $Y_{h(\text{new})}$ must take account of the variation in possible location of the distribution of $Y$ and the variation within the probability distribution of $Y$
\item Prediction limits for a new observations $Y_{h(\text{new})}$ at a given level $X_h$ are obtained by the following theorem: $$ \frac{Y_{h(\text{new})} - \hat{Y}_h}{s[\text{pred]}} \text{ is distributed as } t(n-2) \text{ for the normal error regression model} $$ Note that the studentized statistic uses the point estimator $\hat{Y}_h$ in the numerator rather than the true mean $\expe{Y}_h$ because the true mean is unknown
\item Thus, when the regression parameters are unknown, the $1-\alpha$ prediction limits for a new observation $Y_{h(\text{new})}$ are $$ \hat{Y}_h \pm \tdist{1 - \frac{\alpha}{2}}{n-2} s[\text{pred}] $$ 
\item The variance of this prediction error can be obtained by utilizing the independence of the new observation $Y_{h(\text{new})}$ and the original $n$ sample cases on which $\hat{Y}_h$ is based 
$$ \var{\text{pred}} = \var{Y_{h(\text{new})} - \hat{Y}_h} = \var{Y_{h(\text{new})}} + \var{\hat{Y}_h} = \sigma^2 + \var{\hat{Y}_h} $$ The first term is the variance of the distribution of $Y$ at $X=X_h$ while the second term is the variance of the sampling distribution of $\hat{Y}_h$
\item An unbiased estimator of $\var{\text{pred}}$ is $$ s^2[\text{pred}] = \text{MSE} + s^2[\hat{Y}_h] $$ which can be expressed as 
$$ s^2[\text{pred}] = \text{MSE}\left[1 + \frac{1}{n} + \frac{(X_h - \Xbar)^2}{\sum (X_i - \Xbar)^2} \right] $$ 
\item The prediction interval for $Y_{h(\text{new})}$ is wider than the confidence interval for $\expe{Y_h}$ because both the variability in $\hat{Y}_h$ from sample to sample and the variation within the probability distribution of $Y$ is encountered
\item The prediction interval is wider the further $X_h$ is from $\Xbar$ since the estimate of the mean $\hat{Y}_h$ is less precise as $X_h$ is located farther away from $\Xbar$
\item The prediction limits for a mean response $\expe{Y_h}$ are sensitive to departures from normality of the error terms distributions
\item The confidence coefficient for the prediction limits refers to the taking of repeated samples based on the same set of $X$ values, and calculating prediction limits for $Y_{h(\text{new})}$ for each sample
\item Prediction limits apply for a single prediction based on the sample data
\item Prediction intervals resemble confidence intervals but differ conceptually; a confidence interval represents an inference on a parameter and is an interval that is intended to cover the value of the parameter; a prediction interval is a statement about the value to be taken by a random variable, the new observation $Y_{h(\text{new})}$
\item Suppose the mean of $m$ new observations on $Y$ for a given level of the predictor variable is to be predicted, then the mean of the new $Y$ observations to be predicted is denoted $\bar{Y}_{h(\text{new})}$ and the appropriate $1-\alpha$ prediction limits are, assuming that the new $Y$ observations are independent: $$ \hat{Y}_h \pm \tdist{1 - \frac{\alpha}{2}}{n-2}s[\text{predmean}] $$ where $$ s^2[\text{predmean}] = \frac{\text{MSE}}{m} + s^2[\hat{Y}_h] $$ or equivalently $$ s^2[\text{predmean}] = \text{MSE}\left[ \frac{1}{m} + \frac{1}{n}. + \frac{(X_h - \Xbar)^2}{\sum (X_i - \Xbar)^2}\right] $$ Note that the variance $s^2[\text{predmean}]$ has two components: (1) the variance of the mean of $m$ observations from the probability distribution of $Y$ at $X=X_h$ and (2) the variance of the sampling distribution of $\hat{Y}_h$ 
\item The prediction limits for predicting $m$ new observations on $Y$ are narrower than those for predicting for a single new observation on $Y$ because it involve a prediction of the mean response for $m$ new observations
\end{itemize}

\subsection{Confidence Band for Regression Line}
\begin{itemize}
\item A confidence band for the entire regression line $\expe{Y} = \beta_0 + \beta_1X$ allows one to determine the appropriateness of a fitted regression function
\item The Working-Hotelling $1-\alpha$ confidence band for the regression line has the following two boundary values at any level $X_h$: $$ \hat{Y}_h \pm W\sd{\hat{Y}_h} $$ where $$ W^2 = 2\Fdist{1-\alpha}{n-2} $$ Here $\Fdist{1-\alpha}{n-2}$ denotes the density of the $F$ distribution at $1-\alpha$ confidence with $n-2$ degrees of freedom; this formula for the boundary values is of exactly the same form as the one for the confidence limits for the mean response at $X_h$, except that the $t$ multiple has been replaced by the $W$ multiple
\item The boundary points of the confidence band for the regression line are wider apart the farther $X_h$ is from the mean $\Xbar$ of the $X$ observations; the $W$ multiple will be larger than the $t$ multiple because the confidence band must encompass the entire regression line, whereas the confidence limits for $\expe{Y_h}$ at $X_h$ apply only at the single level $X_h$
\item The boundary values of the confidence band for the regression line define a hyperbola, as seen by replacing $\hat{Y}_h$ and $s[\hat{Y}_h]$ by their definitions $$ b_0 + b_1X \pm W \sqrt{\text{MSE}} \left[ \frac{1}{n} + \frac{(X - \Xbar)^2}{\sum (X_i - \Xbar)^2} \right]^{\frac{1}{2}} $$ 
\item The boundary values of the confidence band for the regression line at any value $X_h$ often are not substantially wider than the confidence limits for the mean response at that single $X_h$ level; with the somewhat wider limits for the entire regression line, one is able to draw conclusions about any and all mean responses for the entire regression line and not just about the mean response at a given $X$ level
\item The confidence band applies to the entire regression line over all real-numbered values of $X$ from $-\infty$ to $\infty$; the confidence coefficient indicates the proportion of time that the estimating procedure will yield a band that covers the entire line, in a long series of samples in which the $X$ observations are kept at the same level as in the actual study; in applications, the confidence band is ignored for that part of the regression line which is not of interest in the problem at hand
\item The confidence coefficient for a limited segment of the band of interest is somewhat higher than $1-\alpha$, so $1-\alpha$ serves then as a lower bound to the confident coefficient
\end{itemize}

\subsection{Analysis of Variance Approach to Regression Analysis}
\begin{itemize}
\item The analysis of variance approach is based on the partitioning of sums of squares and degrees of freedom associated with the response variable $Y$ 
\item Variation is conventionally measured in terms of the deviations of the $Y_i$ around their mean $\hat{Y}$: $$ Y_i - \Ybar $$ 
\item The measure of total variation, denoted by SSTO (total sum of squares), is the sum of the squared deviations $$ \text{SSTO} = \sum (Y_i - \Ybar)^2 $$ If all $Y_i$ observations are the same, SSTO $= 0$; the greater the variation among the $Y_i$ observations, the larger is SSTO
\item When the predictor variable $X$ is utilized, the variation reflecting the uncertainty concerning the variable $Y$ is that of the $Y_i$ observations around the fitted regression line: $$ Y_i - \hat{Y}_i $$ 
\item The measure of variation in the $Y_i$ observations that is present when the predictor variable $X$ is taken into account is the sum of the squared deviations $$ \text{SSE} = \sum (Y_i - \hat{Y}_i)^2 $$ where SSE denotes error sum of squares; if all $Y_i$ observations fall on the fitted regression line, SSE $=0$; the greater the variation of the $Y_i$ observations around the fitted regression line, the larger is SSE
\item Another important deviations is squared deviations $$ \hat{Y}_i - \Ybar $$ SSR, or regression sum of squares, is a sum of squared deviations $$ \text{SSR} = \sum (\hat{Y}_i - \Ybar)^2 $$ Each deviation is simply the difference between the fitted value on the regression line and the mean of the fitted values $\Ybar$
\item If the regression line is horizontal so that $\hat{Y}_i - \Ybar \equiv 0$, then SSR $=0$; otherwise SSR is positive
\item SSR may be considered a measure of that part of the variability of the $Y_i$ which is associated with the regression line; the larger SSR is in relation to SSTO, the greater is the effect of the regression relation in accounting for the total variation in the $Y_i$ observations 
\item The total deviation $Y_i - \Ybar$, used in the measure of the total variation of the observations $Y_i$ without taking the predictor variable into account, can be decomposed into two components: 
$$ \underbrace{Y_i - \Ybar}_{\text{total deviation}} = \underbrace{\hat{Y}_i - \Ybar}_{\text{ deviation of fitted regression value around mean}} + \underbrace{Y_i - \hat{Y}_i}_{\text{deviation around fitted regression line}} $$ These two components are: the deviation of the fitted value $\hat{Y}_i$ around the mean $\Ybar$ and the deviation of the observation $Y_i$ around the fitted regression line
\item This relationship can be summarized as $$ \begin{aligned} \text{SSTO} &= \text{SSR} + \text{SSE} \\ \sum (Y_i - \Ybar)^2 &= \sum (\hat{Y}_i - \Ybar)^2 + \sum (Y_i - \hat{Y}_i)^2 \end{aligned} $$ 
\item Proof; $$ \begin{aligned} \sum (Y_i - \Ybar)^2 &= \sum[(\hat{Y}_i - \Ybar) + (Y_i - \hat{Y}_i)]^2 \\ &= \sum [(\hat{Y}_i - \Ybar)^2 + (Y_i - \hat{Y}_i)^2 + 2(\hat{Y}_i - \Ybar)(Y_i - \hat{Y}_i)] \\ &= \sum (\hat{Y}_i - \Ybar)^2 + \sum (Y_i - \hat{Y}_i)^2 + 2\sum (\hat{Y}_i - \Ybar)(Y_i - \hat{Y}_i) \\ &= \sum (\hat{Y}_i - \Ybar)^2 + \sum (Y_i - \hat{Y}_i)^2 + 2\sum \hat{Y}_i(Y_i - \hat{Y}_i) - 2\Ybar \sum (Y_i - \hat{Y}_i) \\ &= \sum (\hat{Y}_i - \Ybar)^2 + \sum (Y_i - \hat{Y}_i)^2 + 0 - 0 \\ &= \sum (\hat{Y}_i - \Ybar)^2 + \sum (Y_i - \hat{Y}_i)^2 \end{aligned} $$ 
\item There are $n-1$ degrees of freedom associated with SSTO; one degree is lost because the deviations $Y_i - \Ybar$ are subject to one constraint: they must sum to zero; equivalently, one degree is lost because the sample mean $\Ybar$ is used to estimate the population mean
\item There are $n-2$ degrees of freedom associated with SSE because two parameters are estimated in obtaining the fitted values $\hat{Y}_i$
\item SSR has one degree of freedom associated with it; although there are $n$ deviations $\hat{Y}_i - \Ybar$, all fitted values $\hat{Y}_i$ are calculated from the same regression line; two degrees of freedom are associated with a regression line (corresponding to the intercept and slope); one of the degrees is lost because the deviations $\hat{Y}_i - \Ybar$ are subject to a constraint: they must sum to zero
\item Note that the degrees of freedom are additive $$ n-1 = 1 + (n-2) $$ 
\item A sum of squares divided by its associated degrees of freedom is called a mean square (MS)
\item The regression mean square, MSR, is $$ \text{MSR} = \frac{\text{SSR}}{1} = \text{SSR} $$ and the error mean square (MSE) is $$ \text{MSE} = \frac{\text{SSE}}{n-2} $$ 
\item Note that mean squares are not additive 
\item The breakdown of the total sum of squares and associated degrees of freedom are displayed in the form of an analysis of variance table (ANOVA table)
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|} \hline 
Source of variation & SS & df & MS & $\expe{\text{MS}}$ \\ \hline 
Regression & $\text{SSR} = \sum (\hat{Y}_i - \Ybar)^2$ & $1$ &  $\text{MSR} = \frac{\text{SSR}}{1}$ & $\sigma^2 + \beta_1^2\sum (X_i - \Xbar)^2$ \\ \hline 
Error & $\text{SSE} = \sum (Y_i - \hat{Y}_i)^2$ & $n-2$ & $\text{MSE} = \frac{\text{SSE}}{n-2}$ & $\sigma^2$ \\ \hline 
Total & $\text{SSTO} = \sum (Y_i - \Ybar)^2$ & $n-1$  & - & - \\ \hline \end{tabular} \caption*{ANOVA Table for Simple Linear Regression}  \end{table}
\item The total sum of squares can be decomposed as follows: $$ \text{SSTO} = \sum (Y_i - \Ybar)^2 = \sum Y_i^2 - n\Ybar^2 $$ In a modified ANOVA table, the total uncorrected sum of squares, denoted by SSTOU, is defined as $$ \text{SSTOU} = \sum Y_I^2 $$ and the correction for the mean sum of squares, denoted by SS(correction for mean) is $$ \text{SS(correction for mean)} = n\Ybar^2 $$ 
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|} \hline 
Source of variation & SS & df & MS \\ \hline 
Regression & $\text{SSR} = \sum (\hat{Y}_i - \Ybar)^2$ & $1$ &  $\text{MSR} = \frac{\text{SSR}}{1}$  \\ \hline 
Error & $\text{SSE} = \sum (Y_i - \hat{Y}_i)^2$ & $n-2$ & $\text{MSE} = \frac{\text{SSE}}{n-2}$  \\ \hline 
Total & $\text{SSTO} = \sum (Y_i - \Ybar)^2$ & $n-1$  & -  \\ \hline 
Correction for mean & $\text{SS(correction for mean)} = n\Ybar^2$ & 1 & - \\ \hline
Total, uncorrected & $\text{SSTOU} = \sum Y_i^2$ & n & - \\ \hline \end{tabular} \caption*{Modified ANOVA Table for Simple Linear Regression}  \end{table} 
\item The expected value of a mean square is the mean of its sampling distribution; it tells what is being estimated by the mean square $$ \begin{aligned} \expe{\text{ SE}} &= \sigma^2 \\ \expe{\text{MSR}} &= \sigma^2 + \beta_1^2\sum (X_i - \Xbar)^2 \end{aligned} $$ 
\item The mean of the sampling distribution of MSE is $\sigma^2$ whether or not $X$ and $Y$ are linearly related, i.e., whether or not $\beta_1 = 0$; the mean of the sampling distribution of MSR is also $\sigma^2$ when $\beta_1=0$; hence when $\beta_1=0$, the sampling distribution of MSR and MSE are located identically and MSR and MSE will tend to be of the same order of magnitude; when $\beta_1 \neq 0$, the mean of the sampling distribution of MSR is located to the right of that of MSE and hence MSR will tend to be larger than MSE
\item For the simple linear regression case, the analysis of variance provides a test for \hyptest{\beta_1 = 0}{\beta_1 \neq 0} The test statistic for the analysis of variance is denoted by $F^*$ $$ F^* = \frac{\text{MSR}}{\text{MSE}} $$ This suggests that large values of $F^*$ support $H_A$ and values of $F^*$ near $1$ support $H_0$
\item Cochran's Theorem: If all $n$ observations $Y_i$ come from the same normal distribution with mean $\mu$ and variance $\sigma^2$, and SSTO is decomposed into $k$ sums of squares $\text{SS}_r$, each with degrees of freedom $\text{df}_r$, then the $\frac{\text{SS}_r}{\sigma^2}$ terms are independent $\chi^2$ variables with $\text{df}_r$ degrees of freedom if $$ \sum_{r=1}^k \text{df}_r = n-1 $$ 
If $\beta_1 = 0$, so that all $Y_i$ have the same mean $\mu = \beta_0$ and the same variance $\sigma^2$, $\frac{\text{SSE}}{\sigma^2}$ and $\frac{\text{SSR}}{\sigma^2}$ are independent $\chi^2$ variables 
\item The test statistic can be written as follows: $$ F^* = \frac{\frac{\text{SSR}}{\sigma^2}}{1} / \frac{\frac{\text{SSE}}{\sigma^2}}{n-2} = \frac{\text{MSR}}{\text{MSE}} $$ But by Cochran's theorem, when $H_0$ holds: $$ F^* \sim \frac{\chidist{1}}{1} / \frac{\chidist{n-2}}{n-2} \text{ when $H_0$ holds} $$ where the $\chi^2$ variables are independent; thus when $H_0$ holds, $F^*$ is the ratio of two independent $\chi^2$ variables, each divided by its degrees of freedom; this is the definition of an $F$ random variable 
\item Thus when $H_0$ holds, $F^*$ follows the $\Fdist{1}{n-2}$ distribution 
\item Even if $\beta_1 \neq 0$, SSR and and SSE are independent and $\frac{\text{SSE}}{\sigma^2} \sim \chi^2$; however, the condition that both $\frac{\text{SSR}}{\sigma^2}$ and $\frac{\text{SSE}}{\sigma^2}$ are $\chi^2$ random variables requires $\beta_1 = 0$
\item Since the test is upper tail and $F^* \sim \Fdist{1}{n-2}$ when $H_0$ holds, the decision rule is as follows when the risk of a Type $1$ error is to be controlled at $\alpha$ \begin{itemize}
\item If $F^* \leq \Fdist{1-\alpha}{n-2}$, conclude $H_0$
\item If $F^* > \Fdist{1-\alpha}{n-2}$, conclude $H_A$ \end{itemize} where $\Fdist{1-\alpha}{n-2}$ is the $(1-\alpha)100$ percentile of the appropriate $F$ distribution 
\item For a given $\alpha$ level, the $F$ test of $\beta_1 = 0$ versus $\beta_1 \neq 0$ is equivalent algebraically to the two-tailed $t$ test; recall that $\text{SSR} = b_1^2 \sum (X_i - \Xbar)^2$, then
$$ F^* = \frac{\text{SSR} / 1}{\text{SSE} / (n-2)} = \frac{b_1^2 \sum (X_i - \Xbar)^2}{\text{MSE}} $$ But since $s2[b_1] = \text{MSE}/ \sum (X_i - \Xbar)^2$, $$ F^* = \frac{b_1^2}{s^2[b_1]} = \left( \frac{b_1}{s[b_1]}\right)^2 = (t^*)^2 $$ The last step follows because the $t^*$ statistic for testing whether or not $\beta_1 =0$ is $t^* = b_1 / s[b_1]$
\item The following relation between the required percentiles of the $t$ and $F$ distributions for the tests exists: $$ [\tdist{1-\frac{\alpha}{2}}{n-2}]^2 = F_{1-\alpha, 1, n-2} $$ 
\item Thus at any given $\alpha$ level, either the $t$ test or the $F$ test for testing $\beta_1 = 0$ vs $\beta_1 \neq 0$ can be used; whenever one test leads to $H_0$, so will the other and corresponding for $H_A$; the $t$ test, however, is more flexible since it can be used for one-sided alternatives involving $\beta_1 (\leq \geq) 0$ versus $\beta_1 (> <) 0$, while the $F$ test cannot
\end{itemize}

\subsection{General Linear Test Approach}
\begin{itemize}
\item The analysis of variance test of $\beta_1 = 0$ vs $\beta_1 \neq 0$ is an example of the general test for a linear statistical model
\item The general linear test approach for a simple linear regression model involves three steps
\item First, for the simple linear regression model, the full model, or the normal error regression model, is obtained $$ Y_i = \beta_0 + \beta_1X_i + \eps_i \text{~~~ full model} $$ 
This full model is fit and the error sum of squares is obtained (SSE(F)) $$ \text{SSE(F)} = \sum [Y_i - (b_0 + b_1X_i)]^2 = \sum (Y_I - \hat{Y}_i)^2 = \text{SSE} $$ Thus for the full model, the error sum of squares is simply SSE
\item Next, consider $H_0$: \hyptest{\beta_1 = 0}{\beta_1 \neq 0} The model when $H_0$ holds is called the reduced or restricted model; when $\beta_1 = 0$, the model reduces to $$ Y_i = \beta_0 + \eps_i \text{~~~ reduced model} $$ This reduced model is fit and the error sum of squares is obtained (SSE(R)) $$ \text{SSE(R)} = \sum (Y_i - b_0)^2 = \sum (Y_i - \Ybar)^2 = \text{SSTO} $$ 
\item It can be shown that SSE(F) never is greater than SSE(R) because the more parameters there are in the model, the better one can fit the data and the smaller are the deviations around the fitted regression function
\item When SSE(F) is not much less than SSE(R), using the full model does not account for much more of the variability of the $Y_i$ than does the reduced model, in which case the data suggest that the reduced model is adequate (i.e., that $H_0$ holds)
\item A large difference would suggest that $H_A$ holds because the additional parameters in the model do help to reduce substantially the variation of the observations $Y_i$ around the fitted regression function
\item The actual test statistic is a function of SSE(R) $-$ SSE(F): $$ F^* = \frac{\text{SSE(R)} - \text{SSE(F)}}{\text{df}_R - \text{df}_F} / \frac{\text{SSE(F)}}{\text{df}_F} $$ which follows the $F$ distribution when $H_0$ holds; the degrees of freedom $\text{df}_R$ and $\text{df}_F$ are those associated with the reduced and full model sums of squares, respectively
\item The decision rule is: \begin{itemize}
\item If $F^* \leq F_{1-\alpha, \text{df}_R - \text{df}_F, \text{df}_F}$, conclude $H_0$
\item If $F^* > F_{1-\alpha, \text{df}_R - \text{df}_F, \text{df}_F}$, conclude $H_A$ \end{itemize}
\item For testing whether or not $\beta_1 = 0$, the following is stated: $$ \begin{aligned} \text{SSE(R)} &= \text{SSTO} ~~~~~~ \text{SSE(F)} &= \text{SSE} \\ \text{df}_F &= n-1 ~~~~~~ \text{df}_F &= n-2 \end{aligned} $$ 
and thus $$ F^* = \frac{\text{SSTO} - \text{SSE}}{(n-1) - (n-2)} / \frac{\text{SSE}}{n-2} = \frac{\text{SSR}}{1} / \frac{\text{SSE}}{n-2} = \frac{\text{MSR}}{\text{MSE}} $$ which is identical to the analysis of variance test statistic 
\item The general linear test approach can be used for highly complex tests of linear statistical models, as well as for simple tests; the basic steps in summary form are: \begin{enumerate}
\item Fit the full model and obtain the error sum of squares SSE(F) 
\item Fit the reduced model under $H_0$ and obtain the error sum of squares SSE(R)
\item Compute the test statistic and use the decision rule \end{enumerate} 
\end{itemize}

\subsection{Descriptive Measures of Linear Association between $X$ and $Y$}
\begin{itemize}
\item SSTO is a measure of the uncertainty in predicting $Y$ when $X$ is not considered; similarly, SSE measures the variation in the $Y_i$ when a regression model utilizing the predictor variable $X$ is employed 
\item A natural measure of the effect of $X$ in reducing the variation in $Y$, i.e., in reducing the uncertainty in predicting $Y$, is to express the reduction in variation (SSTO $-$ SSE $=$ SSR) as a proportion of the total variation
$$ R^2 = \frac{\text{SSR}}{\text{SSTO}} = 1 - \frac{\text{SSE}}{\text{SSTO}} $$ The measure $R^2$ is called the coefficient of determination; since $0 \leq \text{SSE} \leq \text{SSTO}$, it follows that $$ 0 \leq R^2 \leq 1 $$ 
\item $R^2$ can be interpreted as the proportionate reduction of total variation associated with the use of the predictor variable $X$; thus the larger $R^2$ is, the more variation of $Y$ is reduced by introducing the predictor variable $X$ 
\item When all observations fall on the fitted regression line, then $\text{SSE} = 0$ and $R^2 = 1$; the predictor variable $X$ accounts for all variation in the observations $Y_i$
\item When the fitted regression line is horizontal so that $b_1 = 0$ and $\hat{Y}_i = \Ybar$, then $\text{SSE} = \text{SSTO}$ and $R^2=0$l the predictor variable $X$ is of no help in reducing the variation in the observations $Y_i$ 
\item In practice, $R^2$ is somewhere between $0$ and $1$; the closer it is to $1$, the greater is said to be the degree of association between $X$ and $Y$
\item It is not true that a high coefficient of determination indicates that useful predictions can be made; it is not true that a high coefficient of determination indicates that the estimated regression line is a good fit; it is not true that a coefficient of determination near zero indicates that $X$ and $Y$ are not related 
\item Note that $R^2$ measures only a relative reduction from SSTO and provides no information about absolute precision for estimating a mean response or predicting a new observation; $R^2$ measures the degree of linear association between $X$ and $Y$
\item A measure of linear association between $Y$ and $X$ when both $Y$ and $X$ are random is the coefficient of correlation; this measure is the signed square root of $R^2$ $$ r = \pm \sqrt{R^2} $$ 
A plus or minus sign is attached to this measure according to whether the slope of the fitted regression line is positive or negative; thus, the range of $r$ is: $-1 \leq r \leq 1$
\item The value taken by $R^2$ in a given sample tends to be affected by the spacing of the $X$ observations; SSE is not affected systematically by the spacing of the $X_i$ since, for the normal error regression model, $\var{Y_i} = \sigma^2$ at all $X$ levels; however, the wider the spacing of the $X_i$ in the sample when $b_1 \neq 0$, the greater will tend to be the spread of the observed $Y_i$ around $\Ybar$ and hence the greater SSTO will be; consequently, the wider the $X_i$ are spaced, the higher $R^2$ will tend to be
\item The regression sum of squares SSR is often called the ``explained variation" in $Y$ and the residual sum of squares SSE is called the ``unexplained variation"; the coefficient $R^2$ is then interpreted in terms of the proportion of the total variation in $Y$ (SSTO) which has been ``explained" by $X$; remember that in a regression model, there is no implication that $Y$ necessarily depends on $X$ in a causal or explanatory sense
\item Regression models do not contain a parameter to be estimated by $R^2$ or $r$; these are simply descriptive measures of the degree of linear association between $X$ and $Y$ in the sample observations that may, or may not, be useful in any instance
\end{itemize}

\subsection{Considerations in Applying Regression Analysis}
\begin{itemize}
\item Regression analysis is used to make inferences for the future
\item The validity of the regression application depends upon whether basic causal conditions in the period ahead will be similar to those in existence during the period upon which the regression analysis is based; this caution applies whether mean responses are to be estimated, new observations predicted or regression parameters estimated 
\item In predicting new observations on $Y$, the predictor variable $X$ itself often has to be predicted 
\item If the $X$ level does not fall far beyond the range of the predictor variable observations, one may have reasonable confidence int he application of the regression analysis; on the other hand, if the $X$ level falls far beyond the range of past data, extreme caution should be excises since one cannot be sure that the regression function that fits the past data is appropriate over the wider range of the predictor variable
\item A statistical test that leads to the conclusion that $\beta_1 \neq 0$ does not establish a cause and effect relation between the predictor and response variables; with nonexperimental data, both the $X$ and $Y$ variables may be simultaneously influenced by other variables not in the regression model; on the other hand, the existence of a regression relation in controlled experiments is often good evidence of a cause and effect relation
\item There are special problems when one wants to estimate several mean responses or predict several new observations for different levels of the predictor variable; the confidence coefficients for the limits given before for estimating a mean response and for the prediction limits for a new observation apply only for a single level of $X$ for a given sample
\item When observations on the predictor variable $X$ are subject to measure errors, the resulting parameter estimates are generally no longer unbiased
\end{itemize}

\subsection{Normal Correlation Models}
\begin{itemize}
\item The normal correlation model for the case of two variables is based on the bivariate normal distribution
\item Two variable $Y_1$ and $Y_2$ are jointly normally distributed if the density function of their joint distribution is that for the bivariate normal distribution 
\begin{dmath*}  f(Y_1, Y_2) = \frac{1}{2\pi \sigma_1 \sigma_2 \sqrt{1 - \rho^2_{12}}} \exp \left[ -\frac{1}{2(1 - \rho^2_{12})} \left[ \left(\frac{Y_1 - \mu_1}{\sigma_1}\right)^2 \\ - 2\rho_{12}\left(\frac{Y_1 - \mu_1}{\sigma_1}\right)\left(\frac{Y_2 - \mu_2}{\sigma_2}\right) + \left(\frac{Y_2 - \mu_2}{\sigma_2}\right)^2 \right] \right]  \end{dmath*} 
\item If $Y_1$ and $Y_2$ are jointly normally distributed, it can be shown that their marginal distributions have the following characteristics: \begin{itemize}
\item The marginal distribution of $Y_1$ is normal with mean $\mu_1$ and standard deviation $\sigma_1$: $$ f_1(Y_1) = \frac{1}{\sqrt{2\pi} \sigma_1} \exp\left[ -\frac{1}{2} \left( \frac{Y_1 - \mu_1}{\sigma_1}\right)^2 \right] $$ 
\item The marginal distribution of $Y_2$ is normal with mean $\mu_2$ and standard deviation $\sigma_2$: $$ f_2(Y_2) = \frac{1}{\sqrt{2\pi} \sigma_2} \exp\left[ -\frac{1}{2}\ \left( \frac{Y_2 - \mu_2}{\sigma_2} \right)^2 \right] $$ \end{itemize}
\item If $Y_1$ and $Y_2$ are each normally distributed, they need not be jointly normally distributed 
\item The five parameters of the bivariate normal density function have the following meaning: \begin{itemize}
\item $\mu_1$ and $\sigma_1$ are the mean and standard deviation of the marginal distribution of $Y_1$
\item $\mu_2$ and $\sigma_2$ are the mean and standard deviation of the marginal distribution of $Y_2$
\item $\rho_{12}$ is the coefficient of correlation between the random variables $Y_1$ and $Y_2$ $$ \rho_{12} = \corr{Y_1}{Y_2} = \frac{\sigma_{12}}{\sigma_1 \sigma_2} $$ 
Here, $\sigma_1$ and $\sigma_2$ denote the standard deviations of $Y_1$ and $Y_2$ and $\sigma_{12}$ denotes the covariance $\cov{Y_1}{Y_2}$ between $Y_1$ and $Y_2$ $$ \sigma_{12} = \cov{Y_1}{Y_2} = \expe{(Y_1 - \mu_1)(Y_2 - \mu_2)} $$ Note that $\sigma_{12} \equiv \sigma_{21}$ and $\rho_{12} \equiv \rho_{21}$ \end{itemize} 
\item If $Y_1$ and $Y_2$ are independent, $\sigma_{12} = 0$ and so $\rho_{12} = 0$; if $Y_1$ and $Y_2$ are positively related, $\sigma_{12}$ is positive and so is $\rho_{12}$; if $Y_1$ and $Y_2$ are negatively related, $\rho_{12}$ is negative and so is $\rho_{12}$
\item The coefficient of correlation $\rho_{12}$ can take on any value between $-1$ and $1$ inclusive; it assumes $1$ is the linear relation between $Y_1$ and $Y_2$ is perfectly positive (direct) and $-1$ if it is perfectly negative (inverse) 
\item One principle use of a bivariate correlation model is to make conditional inferences regarding one variable, given the other variable
\item The density function of the conditional probability distribution of $Y_1$ for any given value of $Y_2$ is denoted by $f(Y_1 |Y_2)$ and defined as follows: $$ f(Y_1|Y_2) = \frac{f(Y_1, Y_2)}{f_2(Y_2)} $$ where $f(Y_1, Y_2)$ is the joint density function of $Y_1$ and $Y_2$ and $f_2(Y_2)$ is the marginal density function of $Y_2$
\item When $Y_1$ and $Y_2$ are jointly normally distributed, the conditional probability distribution of $Y_1$ for any given value of $Y_2$ is normal with mean $\alpha_{1|2} + \beta_{12}Y_2$ and standard deviation $\sigma_{1|2}$ and its density function is $$ f(Y_1|Y_2) = \frac{1}{\sqrt{2\pi} \sigma_{1|2}} \exp\left[-\frac{1}{2} \left( \frac{Y_1 - \alpha_{1|2} - \beta_{12}Y_2}{\sigma_{1|2}}\right)^2\right] $$ 
The parameters $\alpha_{1|2}$, $\beta_{12}$ and $\sigma_{1|2}$ of the conditional probability distribution of $Y_1$ are functions of the parameters of the joint probability distribution as follows $$ \begin{aligned} 
\alpha_{1|2} &= \mu_1 - \mu_2\rho_{12}\frac{\sigma_1}{\sigma_2} \\ \beta_{12} &= \rho_{12}\frac{\sigma_1}{\sigma_2} \\ \sigma^2_{1|2} &= \sigma_1^2(1 - \rho^2_{12}) \end{aligned} $$ 
The parameter $\alpha_{1|2}$ is the intercept of the line of regression of $Y_1$ on $Y_2$ and the parameter $\beta_{12}$ is the slope of this line
\item The conditional distribution of $Y_1$, given $Y_2$, is equivalent to the normal error regression model
\item The conditional probability distribution of $Y_2$ for any given value of $Y_1$ is normal with mean $\alpha_{2|1} + \beta_{21}Y_1$ and standard deviation $\sigma_{2|1}$ and its density function is $$ f(Y_2|Y_1) = \frac{1}{\sqrt{2\pi} \sigma_{2|1}} \exp\left[ -\frac{1}{2} \left( \frac{Y_2 - \alpha_{2|1} - \beta_{21}Y_1}{\sigma_{2|1}} \right)^2 \right] $$ The parameters $\alpha_{2|1}$, $\beta_{21}$ and $\sigma_{2|1}$ of the conditional probability distributions of $Y_2$ are functions of the parameters of the joint probability distribution as follows $$ \begin{aligned} \alpha_{2|1} &= \mu_2 - \mu_1\rho_{12}\frac{\sigma_2}{\sigma_1} \\ \beta_{21} &= \rho_{12}\frac{\sigma_2}{\sigma_1} \\ \sigma^2_{2|1} &= \sigma_2^2(1-\rho^2_{12}) \end{aligned} $$ 
\item The conditional probability distribution of $Y_1$ for any given value of $Y_2$ is normal 
\item The means of the conditional probability distributions of $Y_1$ fall on a straight line and hence are a linear function of $Y_2$ $$ \expe{Y_1|Y_2} = \alpha_{1|2} + \beta_{12}Y_2 $$ Here $\alpha_{1|2}$ is the intercept parameter and $\beta_{12}$ the slope parameter; thus the relation between the conditional means and $Y_2$ is given by a linear regression function 
\item All conditional probability distributions of $Y_1$ have the same standard deviation $\sigma_{1|2}$
\item Suppose a random sample of observations $(Y_1, Y_2)$ was to be selected from a bivariate normal population and conditional inferences about $Y_1$, given $Y_2$, was to be made, then the normal error regression model is entire applicable because the $Y_1$ observations are independent and the $Y_1$ observations when $Y_2$ is considered given or fixed are normally distributed with mean $\expe{Y_1|Y_2} = \alpha_{1|2} + \beta_{12}Y_2$ and constant variance $\sigma^2_{1|2}$
\item All conditional inferences with these correlation models can be made by means of the usual regression methods 
\item If $Y_1$ and $Y_2$ are not bivariate normal, but if $Y_1=Y$ and $Y_2=X$ are random variables, then all results on estimation, testing and prediction obtained the regression model apply if the following conditions hold: \begin{itemize} 
\item The conditional distributions of the $Y_i$, given $X_i$, are normal and independent, with conditional means $\beta_0 + \beta_1X_i$ and conditional variance $\sigma^2$
\item The $X_i$ are independent random variables whose probability distribution $g(X_i)$ does not involve the parameters $\beta_0$, $\beta_1$ and $\sigma^2$ \end{itemize} 
These conditions require only that the regression model is appropriate for each conditional distribution of $Y_i$ and that the probability distribution of the $X_i$ does not involve the repression parameters 
\item Two distinct regressions are involved in a bivariate normal model, that of $Y_1$ on $Y_2$ when $Y_2$ is fixed and that of $Y_2$ on $Y_1$ when $Y_1$ is fixed; in general, the two regression lines are not the same
\item When interval estimates for the conditional correlation models are obtained, the confidence coefficient refers to repeated samples where pairs of observations ($Y_1, Y_2$) are obtained from the bivariate normal distribution 
\item A principal use of the bivariate normal correlation model is to study the relationship between two variables; in a bivariate normal model, the parameter $\rho_{12}$ provides information about the degree of the linear relationship between the two variables $Y_1$ and $Y_2$
\item The maximum likelihood estimator of $\rho_{12}$, denoted by $r_{12}$, is given by $$ r_{12} = \frac{\sum (Y_{i1} - \Ybar_1)(Y_{i2} - \Ybar_2)}{[\sum (Y_{i1} - \Ybar_1)^2 \sum (Y_{i2} - \Ybar)^2]^{\frac{1}{2}}} $$ This estimator is called the Pearson product-moment correlation coefficient; if is a biased estimator of $\rho_{12}$ (unless $\rho_{12} = 0$ or $1$), but the bias is small when $n$ is large 
\item The range of $r_{12}$ is $-1 \leq r_{12} \leq 1$; generally, values of $r_{12}$ near $1$ indicate a strong positive (direct) linear association between $Y_1$ and $Y_2$ whereas values of $r_{12}$ near $-1$ indicate a strong negative (indirect) linear association; values of $r_{12}$ near $0$ indicate little or no linear association between $Y_1$ and $Y_2$ 
\item When the population is bivariate normal, it is desired to test whether the coefficient of correlation is zero \hyptest{ \rho_{12} = 0}{\rho_{12} \neq 0} When $Y_1$ and $Y_2$ are jointly normally distributed, $\rho_{12} = 0$ implies that $Y_1$ and $Y_2$ are independent 
\item The test statistic for testing these hypotheses is $$ t^* = \frac{r_{12} \sqrt{n-2}}{\sqrt{1-r_{12}^2}} $$ If $H_0$ holds, $t^*$ follows the $t_{n-2}$ distribution; the appropriate decision rule to control the Type I error at $\alpha$ is \begin{itemize} 
\item If $|t^*| \leq \tdist{1 - \frac{\alpha}{2}}{n-2}$, conclude $H_0$ 
\item If $|t^*| > \tdist{1 - \frac{\alpha}{2}}{n-2}$, conclude $H_A$ \end{itemize} 
\item Since the sampling distribution of $r_{12}$ is complicated with $\rho_{12} \neq 0$, interval estimation of $\rho_{12}$ is usually carried out by means of an approximate procedure based on a Fisher $z$ transformation 
$$ z' = \frac{1}{2} \log_e \left(\frac{1+r_{12}}{1 - r_{12}} \right) $$ When $n$ is large, the distribution of $z'$ is approximately normal with approximate mean and variance: $$ \begin{aligned} \expe{z'} &= \xi = \frac{1}{2}\log_e \left( \frac{1 + \rho_{12}}{1 - \rho_{12}} \right) \\ \var{z'} &= \frac{1}{n-3} \end{aligned} $$ 
\item When the sample size is large, the standardized statistic: $$ \frac{z' - \xi}{\sd{z'}} $$ is approximately a standard normal variable; therefore, approximate $1-\alpha$ confidence limits for $\xi$ are $$ z' \pm \zdist{1 - \frac{\alpha}{2}} \sd{z'} $$ where $\zdist{1-\frac{\alpha}{2}}$ is the $(1-\frac{\alpha}{2})100$ percentile of the standard normal distribution; the $1-\alpha$ confidence limits for $\rho_{12}$ are then obtained by transforming the limits on $\xi$ using the appropriate mean of $z'$ above 
\item A confidence interval for $\rho_{12}$ can be employed to test whether or not $\rho_{12}$ has a specified value by noting whether or not the specified value falls within the confidence limits 
\item It can be shown that the square of the coefficient of correlation, $\rho^2_{12}$, measures the relative reduction in the variability of $Y_2$ associated with the use of variable $Y_1$; note that $$ \sigma^2_{1|2} = \sigma_1^2(1-\rho^2_{12}) ~~~~~~ \rho^2_{2|1} = \sigma^2_2(1-\rho^2_{12}) $$ Then these expressions can be rewritten as $$ \rho_{12}^2 = \frac{\sigma_1^2 - \sigma_{1|2}^2}{\sigma_1^2} = \frac{\sigma_2^2 - \sigma^2_{2|1}}{\sigma^2_2} $$ 
$\rho^2_{12}$ measures how much smaller relatively is the variability in the conditional distributions of $Y_1$, for any given level of $Y_2$, than is the variability in the marginal distribution of $Y_1$; thus, $\rho^2_{12}$ measures the relative reduction in the variability of $Y_1$ associated with the use of $Y_2$; correspondingly, $\rho^2_{12}$ also measures the relative reduction in the variability of $Y_2$ associated with the use of variable $Y_1$
\item The limits of $\rho^2_{12}$ are $0 \leq \rho^2_{12} \leq 1$; the limiting value $\rho^2_{12} = 0$ occurs when $Y_1$ and $Y_2$ are independent, so that the variances of each variable in the conditional probability distributions are then no smaller than the variance in the marginal distribution; the limiting value $\rho^2_{12} = 1$ occurs when there is no variability in the conditional probability distributions for each variable, so perfect predictions of either variable can be made from each other 
\item The interpretation of $\rho^2_{12}$ as measuring the relative reduction in the conditional variances as compared with the marginal variance is valid for the case of a bivariate normal population, but not for many other bivariate populations
\item Confidence limits for $\rho^2_{12}$ can be obtained by squaring the respective confidence limits for $\rho_{12}$, provided the latter limits do not differ in sign
\item When the joint distribution of two random variables $Y_1$ and $Y_2$ differs considerably from the bivariate normal distribution, transformations of the variables $Y_1$ and $Y_2$ may be sought to make the joint distribution of the transformed variables approximately bivariate normal
\item When no appropriate transformations can be found, a nonparametric rank correlation procedure may be useful for making inferences about the association between $Y_1$ and $Y_2$
\item The Spearman rank correlation coefficient is calculated as follows: first the observations on $Y_1$ are expressed in ranks from $1$ to $n$ and denoted by $R_{i1}$; similarly, the observations on $Y_2$ are ranked, denoted by $R_{i2}$; the Spearman rank correlation coefficient, $r_s$ , is then defined as the ordinary Pearson product-moment correlation coefficient based on the rank data: $$ r_s = \frac{\sum (R_{i1} - \bar{R}_1)(R_{i2} - \bar{R}_2)}{[\sum (R_{i1} - \bar{R}_1)^2 \sum (R_{i2} - \bar{R}_2)^2]^{\frac{1}{2}}} $$ Here $\bar{R}_1$ is the mean of the ranks $R_{i1}$ and $\bar{R}_2$ is the mean of the ranks $R_{i2}$
\item Note that $$ \bar{R}_1 = \bar{R}_2 = \frac{n+1}{2} $$ since the ranks are the integers $1,\dots,n$
\item The Spearman rank correlation coefficient takes on values between $-1$ and $1$ inclusive: $-1 \leq r_s \leq 1$; the coefficient $r_s$ equals $1$ when the ranks for $Y_1$ are identical to those for $Y_2$; in that case, there is perfect association between the ranks for the two variables; the coefficient $r_s$ equals $-1$ when the case with rank $1$ for $Y_1$ has rank $n$ for $Y_2$, the case with rank $2$ for $Y_1$ has rank $n-1$ for $Y_2$ and so on; here, there is perfect inverse association between the ranks for the two variables; when there is little, if any, association between the ranks of $Y_1$ and $Y_2$, the Spearman rank correlation coefficient tends to have a value near zero
\item The Spearman rank correlation coefficient can be used the test the alternatives: \begin{itemize} 
\item $H_0$: There is no association between $Y_1$ and $Y_2$ 
\item $H_A$: There is an association between $Y_1$ and $Y_2$ \end{itemize} 
A two-sided test is conducted here since $H_A$ includes either positive or negative association
\item When the alternative $H_A$ is: $$ H_A: \text{ There is positive (negative) association between $Y_1$ and $Y_2$} $$ 
an upper-tail (power-tail) one-sided test is conducted 
\item The probability distribution of $r_s$ under $H_0$ is based on the condition that, for any ranking of $Y_1$, all rankings of $Y_2$ are equally likely when there is no association between $Y_1$ and $Y_2$
\item When the sample size $n$ exceeds $10$, the test can be carried out approximately by using the following test statistic $$ t^* = \frac{r_s \sqrt{n-2}}{\sqrt{1-r_s^2}} $$ based on the $t$ distribution with $n-2$ degrees of freedom
\item Another nonparametric rank procedure similar to Spearman's $r_s$ is Kendall's $\tau$; this statistic also measures how far the rankings of $Y_1$ and $Y_2$ differ from each other, but in a somewhat different way than the Spearman rank correlation coefficient
\end{itemize}


\section{Diagnostics and Remedial Measures}
\subsection{Diagnostics for Predictor Variable}
\begin{itemize} 
\item Diagnostic information about the predictor variable tell if there are any outlying $X$ values that could influence the appropriateness of the fitted regression function 
\item A dot plot is helpful when the number of observations in the data set is not large 
\item A sequence plot is useful when the data is obtained in a sequence, such as over time or for adjacent geographic areas 
\item A stem and leaf plot provides information similar to a frequency histogram but by displaying last digits explicitly 
\item A box plot shows the minimum, maximum, first quartile, third quartile and the median of the data set; this visualization is particularly helpful when there are many observations in the data set 
\end{itemize} 

\subsection{Residuals}
\begin{itemize}
\item Direct diagnostic plots for the response variable $Y$ are ordinarily not too useful in regression analysis because the values of the observations on the response variable are a function of the level of the predictor variable 
\item The residual $e_i$ is the difference between the observed value $Y_i$ and the fitted value $\hat{Y}_i$ $$ e_i = Y_i - \hat{Y}_i$$ The residual may be regarded as the observed error, in distinction to the unknown true error $\eps_i$ in the regression model $$ \eps_i = Y_i - \expe{Y_i} $$ 
\item If the model is appropriate for the data at hand, the observed residuals $e_i$ should then reflect the properties assumed for the $\eps_i$
\item The mean of the $n$ residuals $e_i$ for the simple linear regression model is $$ \bar{e} = \frac{\sum e_i}{n} = 0 $$ where $\bar{e}$ denotes the mean of the residuals; since $\bar{e}$ is always $0$, it provides no information as to whether the true errors $\eps_i$ have expected value $\expe{\eps_i} = 0$ 
\item The variance of the $n$ residuals $e_i$ for the simple linear regression model is $$ s^2 = \frac{\sum (e_i - \bar{e})^2}{n-2} = \frac{\sum e_i^2}{n-2} = \frac{\text{SSE}}{n-2} = \text{MSE} $$ If the model is appropriate, the MSE is an unbiased estimator of the variance of the error terms $\sigma^2$ 
\item The residuals for the regression model are subjected to two constraints: the sum of the $e_i$ must be $0$ and the products $X_ie_i$ must sum to $0$
\item When the sample size is large in comparison to the number of parameters in the regression model, the dependency effect among the residuals $e_i$ is relatively unimportant and can be ignored for most purposes
\item It is helpful to consider the standardization of the residuals for residual analysis as it can identify outlying observations 
\item The semistudentized residual is defined as follows: $$ e_i^* = \frac{e_i - \bar{e}}{\sqrt{\text{MSE}}} = \frac{e_i}{\sqrt{\text{MSE}}} $$ 
\item The following can be noted as departures from the simple linear regression model \begin{itemize} 
\item The regression function is not linear 
\item The error terms do not have constant variance
\item The error terms are not independent 
\item The model fits all but one or a few outlier observations 
\item The error terms are not normally distributed 
\item One or several important predictor variables have been omitted from the model \end{itemize} 
\end{itemize} 

\subsection{Diagnostics for Residuals}
\begin{itemize}
\item The following plots of residuals (or semistudentized residuals) can be utilized to diagnose any departures from the simple linear regression model \begin{itemize} 
\item Plot of residuals against predictor variable
\item Plot of absolute or squared residuals against predictor variable 
\item Plot of residuals against fitted values 
\item Plot of residuals against time or other sequence 
\item Plots of residuals against omitted predictor variables 
\item Box plot of residuals 
\item Normal probability plot of residuals \end{itemize} 
\item Whether a linear regression function is appropriate for the data being analyzed can be studied from a residual plot against the predictor variable, or equivalently, from a residual plot against the fitted values 
\item Nonlinearity of the regression function can also be studied from a scatter plot, but this plot is not always as effective as a residual plot 
\item In general, the residual plot is to be preferred over the scatter plot due to its advantages \begin{itemize} 
\item The residual plot can be used for examining other facets of the aptness of the model 
\item When the scaling of the scatter plot places $Y_i$ observations close to the fitted values $\hat{Y}_i$, it becomes difficult to study the appropriateness of a linear regression function from the scatter plot \end{itemize} 
\item Note: A plot of residuals against the fitted values $\hat{Y}$ provides equivalent information as a plot of residuals against $X$ for the simple linear regression model because the fitted values $\hat{Y}_i$ are a linear function of the values $X_i$ for the predictor variable and thus, only the $X$ scale values, not the basic pattern of the plotted points, are affected by whether the residual plot is against the $X_i$ or the $\hat{Y}_i$  
\item Plots of the residuals against the predictor variable or against the fitted values are not only helpful to study whether a linear regression function is appropriate but also to examine whether the variance of the error terms is constant 
\item Plots of the absolute values of the residuals or of the squared residuals against the predictor variable $X$ or against the fitted values $\hat{Y}$ are also useful for diagnosing nonconstancy of the error variance since the signs of the residuals are not meaningful for examining the constancy of the error variance
\item These plots are especially useful when there are not many cases in the data set because plotting of either the absolute or squared residuals places all of the information on changing magnitudes of the residuals above the horizontal zero line so that one can more readily see whether the magnitude of the residuals (irrespective of sign) is changing with the level of $X$ or $\hat{Y}$
\item Residual outliers can be identified from residual plots against $X$ or $\hat{Y}$, as well as from box plots, stem and leaf plots and dot plots of the residuals 
\item Plotting of semistudentized residuals is helpful for distinguishing outlying observations, since it then becomes easy to identify residuals that lie many standard deviations away from zero
\item A rough rule of thumb when the number of cases is large is to consider semistudentized residuals with absolute value of four or more to be outliers
\item Outliers can create great difficulty - a major reason for discarding it is that under the least squares method, a fitted line may be pulled disproportionately toward an outlying observation because the sum of the squared deviations is minimized, causing a misleading fit if indeed the outlying observation resulted from a mistake or other extraneous cause 
\item When a linear regression model is fitted to a data set with a small number of cases and an outlier is present, the fitted regression can be so distorted by the outlier that the residual plot may improperly suggest a lack of fit for the linear regression model, in addition to flagging the model 
\item Whenever data are obtained in a time sequence or some other type of sequence, it is a good idea to prepare a sequence plot of the residuals to see if there is any correlation between error terms that are near each other in the sequence 
\item When the error terms are independent, the residuals in a sequence plot are to be expected to fluctuate in a more or less random pattern around the base line $0$; lack of randomness can take the form of too much or too little alternation of points around the zero line 
\item Small departures from normality do not create any serious problems; major departures should be of concern, such as the normality of the error terms 
\item A box plot of the residuals is helpful for obtaining summary information about the symmetry of the residuals and about possible outliers; a histogram, dot plot or stem and leaf plot of the residuals can also be useful for detecting gross departures from normality 
\item When the number of cases is reasonably large, it can be useful to compare actual frequencies of the residuals against expected frequencies under normality 
\item A normal probability plot of the residuals can also shed some information about the normality of error terms; here each residual is plotted against its expected value under normality; a plot that is nearly linear suggests agreement with normality, whereas a plot that departs substantially from linearity suggests that the error distribution is not normal \begin{itemize} 
\item To find the expected values of the ordered residuals under normality, note that the expected value of the error terms for the regression model is zero and that the standard deviation of the error terms is estimated by $\sqrt{\text{MSE}}$ 
\item For a normal random variable with mean $0$ and estimated standard deviation $\sqrt{\text{MSE}}$, a good approximation of the expected value of the $k$th smallest observation in a random sample of $n$ is 
$$ \sqrt{\text{MSE}}\left( z\left(\frac{k - .375}{n + .25}\right)\right) $$ where $z(A)$ is the $(A)100$ percentile of the standard normal distribution \end{itemize} 
\item When the distribution of the error terms departs substantially from normality: \begin{itemize} 
\item If the error term distribution is highly skewed to the right in the normal probability plot, the plot is shaped concave-upward
\item If the error term distribution is highly skewed to the left in the normal probability plot, the plot is shaped concave-downward 
\item If the error term distribution is symmetrical but has heavy tails in the normal probability plot, one side is concave-upward while the other is concave-downward; here the distribution has higher probabilities in the tails than a normal distribution \end{itemize} 
\item The analysis for model departures with respect to normality is more difficult than that for other types of departure because random variation can be particularly mischievous when studying the nature of a probability distribution unless the sample size is quite large; even worse, other types of departures can and do affect the distribution of the residuals 
\item Residuals should also be plotted against variables omitted from the model that might have important effects on the response to determine whether there are any other key variables that could provide important additional descriptive and predictive power to the model 
\item Note that in actuality, several types of model departures may occur together 
\item Although graphic analysis of residuals is only an informal method of analysis, in many cases it suffices for examining the aptness of a model 
\item Model misspecification due to either nonlinearity or the omission of important predictor variables tends to be serious, leading to biased estimates of the regression parameters and error variance 
\item Nonconstancy of error variance tends to be less serious, leading to less efficient estimates and invalid error variance estimates 
\item The presence of outliers can be serious for smaller data sets when their influence is large 
\item The nonindependence of error terms results in estimators that are unbiased but whose variances are seriously biased 
\end{itemize} 

\subsection{Overview for Tests Involving Residuals}
\begin{itemize} 
\item A runs test is frequently used to test for lack of randomness in the residuals arranged in time order; another, specifically designed for lack of randomness in least squares residuals, is the Durbin-Watson test 
\item When a residual plot gives the impression that the variance may be increasing or decreasing in a systematic manner related to $X$ or $\expe{Y}$, a simple test is based on the rank correlation between the absolute values of the residuals and the corresponding values of the predictor variable; two other simple tests for constancy of the error variance are the Brown-Forsythe test and the Breusch-Pagan test
\item A simple test for identifying an outlier observation involves fitting a new regression line to the other $n-1$ observations; the suspect observation can now be regarded as a new observation; the probability that in $n$ observations, a deviation from the fitted line as great as that of the outlier will be obtained by chance can be calculated; if this probability is sufficiently small, the outlier can be rejected as not having come from the same population as the other $n-1$ observations; otherwise, the outlier is retained 
\item Goodness of fit tests can be used for examining the normality of the error terms, such as the chi-square test or the Kolmogorov-Smirnov test and its modification, as well as the Lilliefors test; a simple test based on the normal probability plot of the residuals is also useful 
\end{itemize} 

\subsection{Correlation Test for Normality}
\begin{itemize}
\item A formal test for normality of the error terms can be conducted by calculating the coefficient of correlation between the residuals $e_i$ and their expected values under normality 
\item A high value of the correlation coefficient is indicative of normality 
\item If the observed coefficient of correlation is at least as large as the tabled value of the expected value for a given $\alpha$ level, one can conclude that the error terms are reasonably normally distributed 
\item The correlation test for normality shown here is simpler than the Shapiro-Wilk test, which can be viewed as being based approximately also on the coefficient of correlation between the ordered residuals and their expected values under normality 
\end{itemize} 

\subsection{Tests for Constancy of Error Variance}
\begin{itemize}
\item The Brown-Forsythe test and the Breusch-Pagan test can both be used to ascertain whether the error terms have constant variance
\item Brown-Forsythe Test \begin{itemize} 
\item The Brown-Forsythe test does not depend on normality of the error terms and is thus robust against serious departures from normality, in the sense that the nominal significance level remains approximately correct when the error terms have equal variances even if the distribution of the error terms is far from normal
\item The Brown-Forsythe test is applicable to simple linear regression when the variance of the error terms either increases or decreases with $X$; the sample size needs to be large enough so that the dependencies among the residuals can be ignored 
\item The test is based on the variability of the residuals; the larger the error variance, the larger the variability of the residuals will tend to be 
\item To conduct the Brown-Forsythe test, divide the data set into two groups, according to the level of $X$, so that one group consists of cases where the $X$ level is comparatively low and the other group consists of cases where the $X$ level is comparatively high
\item If the error variance is either increasing or decreasing with $X$, the residuals in one group will tend to be more variable than those in the other group; equivalently, the absolute deviations of the residuals around their group mean will tend to be larger for one group than for the other group
\item The Brown-Forsythe test consists simply of the two-sample $t$ test based on the test statistic to determine whether the mean of the absolute deviations of the residuals around the median for one group differs significantly from the mean absolute deviations os the residuals around the median of the other group 
\item Let $e_{i1}$ denote the $i$th residual for group $1$ and $e_{i2}$ the $i$th residual for group $2$ and let $n_1$ and $n_2$ denote the sample sizes of the two groups such that $n = n_1 + n_2$
\item Let $\bar{e}_1$ and $\bar{e}_2$ be the medians of the residuals in the two groups; the Brown-Forsythe test uses the absolute deviations of the residuals around their group median, to be denoted by $d_{i1}$ and $d_{i2}$: $$ d_{i1} = \abs{e_{i1} - \bar{e}_1} ~~~~~ d_{i2} = \abs{e_{i2} - \bar{e}_2} $$ 
\item The two-sample $t$ test statistic is $$ t^*_{BF} = \frac{\bar{d}_1 - \bar{d}_2}{s\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} $$ where $\bar{d}_1$ and $\bar{d}_2$ are the sample means of the $d_{i1}$ and $d_{i2}$ respectively and the pooled variance $s^2$ is $$ s^2 = \frac{\sum (d_{1i} - \bar{d}_1)^2 + \sum (d_{i2} - \bar{d}_2)^2}{n-2} $$ 
\item If the error terms have constant variance and $n_1$ and $n_2$ are sufficiently large, $t^*_{BF}$ follows the $t_{n-2}$ distribution; large absolute values of $t^*_{BF}$ indicates that the error terms do not have constant variance 
\item If the data set contains many cases, the two-sample $t$ test for constancy of error variance can be conducted after dividing the cases into three or four groups, according to the level of $X$, and using the two extreme groups 
\item A robust test for constancy of the error variance is desirable because nonnormality and lack of constant variance go hand in hand 
\end{itemize} 
\item Breutch-Pagan Test \begin{itemize} 
\item The Breutch-Pagan test, a large sample test, assumes that the error terms are independent and normally distributed and that the variance of the error terms $\eps_i$, denoted by $\sigma^2_i$, is related to the level of $X$ such that $$ \log_e \sigma_i^2 = \gamma_0 + \gamma_1X_i $$ This implies that $\sigma^2_i$ either increases or decreases with the level of $X$, depending on the sign of $\gamma_1$ 
\item Constancy of error variance corresponds to $\gamma_1 = 0$ 
\item The test of $H_0: \gamma_1 = 0$ vs $H_A: \gamma_1 \neq 0$ is carried out by means of regressing the squared residuals $e_i^2$ against $X_i$ in the usual manner and obtaining the regression sum of squares, $\text{SSR}^*$
\item The test statistic $\chidist{BP}$ is as follows: $$ \chidist{BP} = \frac{\text{SSR}^*}{2} / \left( \frac{\text{SSE}}{n}\right)^2 $$ where $\text{SSR}^*$ is the regression sum of squares when regressing $e^2$ on $X$ and SSE is the error sum of squares when regressing $Y$ on $X$ 
\item If $H_0: \gamma_1 = 0$ holds and $n$ is reasonably large, $\chi^2_{BP}$ follows the $\chidist{1}$ distribution 
\item Large values of of $\chidist{BP}$ lead to the conclusion $H_A$, that the error variance is not constant
\item The Breusch-Pagan test can be modified to allow for different relationships between the error variance and the level of $X$ 
\item The test statistic was developed independently by Cook and Weisberg and the test is sometimes referred to as the Cook-Weisberg test
\end{itemize} 
\end{itemize} 

\subsection{$F$ Test for Lack of Fit}
\begin{itemize} 
\item The $F$ test for lack of fit is for ascertaining whether a linear regression function is a good fit for the data
\item The lack of fit test assumes that the observations $Y$ for given $X$ are (1) independent and (2) normally distributed, and that (3) the distributions of $Y$ have the same variance $\sigma^2$ 
\item The lack of fit test requires repeat observations at one or more $X$ levels
\item Repeat trials for the same level of the predictor variable are called replications and the resulting observations are called replicates
\item The different $X$ levels in a study, whether or not replicated observations are present, are denoted as $X_1, \dots, X_c$; the number of replicates for the $j$th level of $X$ is denoted as $n_j$ and therefore the total number of observations $n$ is $$ n = \sum_{j=1}^c n_j $$ 
The observed value of the response variable for the $i$th replicate for the $j$th level of $X$ is denoted by $Y_{ij}$, where $i=1,\dots,n_j$ and $j=1,\dots,c$
\item The general linear test approach begins with the specification of the full model, making the same assumptions as the simple linear regression model except for assuming a linear regression function, the subject of the test $$ Y_{ij} = \mu_i + \eps_{ij} $$ where $\mu_j$ are parameters $j=1,\dots,c$ and $\eps_{ij}$ are independent $N(0,\sigma^2)$ 
\item Since the error terms have expectation zero, it follows that $ \expe{Y_{ij}} = \mu_j$ and so the parameter $\mu_j$ is the mean response when $X=X_j$ 
\item Note that the full model here makes no restrictions on the mean $\mu_j$, whereas in the regression model, the mean responses are linear related to $X$ (i.e., $\expe{Y} = \beta_0 + \beta_1X$)
\item To fit the full model to the data, the least squares or maximum likelihood estimators for the parameters $\mu_j$ is needed, which is simply the sample means $\bar{Y}_j$ 
\item The estimated expected value for observation $Y_{ij}$ is $\bar{Y}_j$ and the error sum of squares for the full model therefore is $$ \text{SSE}(F) = \sum_j \sum_i (Y_{ij} - \bar{Y}_j)^2 = \text{SSPE} $$ This is called the pure error sum of squares, or SSPE
\item Note that SSPE is made up of the sums of squared deviations at each $X$ level; at level $X=X_j$, this sum of squared deviations is $\sum_i (Y_{ij} - \bar{Y}_j)^2$ 
\item Note that any $X$ level with no replications make no contribution to SSPE because $\bar{Y}_j = Y_{1j}$
\item The degrees of freedom associated with SSPE is the sum of the component degrees of freedom: $$ df_F = \sum_j (n_j - 1) = \sum_j n_j - c = n-c $$ 
\item The general linear test approach also requires consideration of the reduced model under $H_0$; the alternatives are 
$$ \begin{aligned} H_0 &: \expe{Y} = \beta_0 + \beta_1X \\ H_A &: \expe{Y} \neq \beta_0 + \beta_1X \end{aligned} $$ and so $H_0$ postulates that $\mu_j$ in the full model is linear related to $X_j$: $\mu_j = \beta_0 + \beta_1X_j$ 
\item The reduced model under $H_0$ is $$ Y_{ij} = \beta_0 + \beta_1X_j + \eps_{ij} $$ 
\item The error sum of squares for the reduced model is the usual error sum of squares SSE $$ \text{SSE}(R) = \sum \sum (Y_{ij} - (b_0 + b_1X_j)^2) = \sum \sum (Y_{ij} - \hat{Y}_{ij})^2 = \text{SSE} $$ 
\item The degrees of freedom associated with SSE(R) is $$ df_R = n-2 $$ 
\item The general linear test statistic here is $$ F^* = \frac{\text{SSE}(R) - \text{SSE}(F)}{df_R - df_F} / \frac{\text{SSE}(F)}{df_F} = \frac{\text{SSE} - \text{SSPE}}{(n-2) - (n-c)} / \frac{\text{SSPE}}{n-c} $$ 
\item The difference between the two error sums of squares is called the lack of fit sum of squares and is denoted by SSLF $$ \text{SSLF} = \text{SSE} - \text{SSPE} $$
\item The test statistic can then be expressed as $$ F^* = \frac{\text{SSLF}}{c-2} / \frac{\text{SSPE}}{n-c} = \frac{\text{MSLF}}{\text{MSPE}} $$ where MSLF denotes the lack of fit mean square and MSPE denotes the pure error mean square
\item Large values of $F^*$ lead to rejection of $H_0$ in the general linear test; the decision rules are as follows: $$ \begin{aligned} \text{If } F^* &\leq \Fdist{1-\alpha}{c-2, n-c}, \text{ conclude } H_0 \\ \text{If } F^* &> \Fdist{1-\alpha}{c-2, n-c}, \text{ conclude } H_A \end{aligned} $$ 
\item The error sum of squares SSE can be decomposed as follows $$ \text{SSE} = \text{SSPE} + \text{SSLF} $$ which follows from the identity $$ \underbrace{Y_{ij} - \hat{Y}_{ij}}_{\text{error deviation}} = \underbrace{Y_{ij} - \bar{Y}_j}_{\text{pure error deviation}} + \underbrace{\var{Y}_j - \hat{Y}_{ij}}_{\text{lack of fit deviation}} $$ This identity shows that the error deviations in SSE are made up of a pure error component and a lack of fit component 
\item When this is squared and summed over all observations, the following is achieved $$ \begin{aligned} \sum \sum (Y_{ij} - \hat{Y}_{ij})^2 &= \sum \sum (Y_{ij} - \bar{Y}_{j})^2 + \sum \sum (\bar{Y}_j - \hat{Y}_{ij})^2 \\ \text{SSE} &= \text{SSPE} + \text{SSLF} \end{aligned} $$ Thus the lack of fit sum of squares can then be defined as $$ \text{SSLF} = \sum \sum (\bar{Y}_j - \hat{Y}_{ij})^2 $$ Since all $Y_{ij}$ observations at the level $X_j$ have the same fitted value, denoted by $\hat{Y}_j$, this can be stated equivalently as $$ \text{SSLF} = \sum_j n_j (\bar{Y}_j - \hat{Y}_j)^2 $$ 
\item If the linear regression function is appropriate, then the means $\bar{Y}_j$ will be near the fitted values $\hat{Y}_j$ calculated from the estimated linear regression function and SSLF will be small; if the linear regression function is not appropriate, the means $\bar{Y}_j$ will not be near the fitted values calculated from the estimated linear regression function and SSLF will be large 
\item There are $c-2$ degrees of freedom associated with SSLF because there are $c$ means $\bar{Y}_j$ in the sum of squares and 2 degrees of freedom are lost in estimating the parameters $\beta_0$ and $\beta_1$ of the linear regression function to obtain the fitted values $\hat{Y}_j$ 
\item An ANOVA table can be constructed for the decomposition of SSE 
$$ \begin{tabular}{|c|c|c|c|} \hline 
Source of Variation & Sum of Squares & df & Mean Squares \\ \hline 
Regression & $\text{SSR} = \sum \sum (\hat{Y}_{ij} - \bar{Y})^2$ & $1$ & $\text{MSR} = \frac{\text{SSR}}{1}$ \\ \hline
Error & $\text{SSE} = \sum \sum (Y_{ij} - \hat{Y}_{ij})^2$ & $n-2$ & $\text{MSE} = \frac{\text{SSE}}{n-2}$ \\ \hline
Lack of Fit & $\text{SSLF} = \sum \sum (\bar{Y}_j - \hat{Y}_{ij})^2$ & $c-2$ & $\text{MSLF} = \frac{\text{SSLF}}{c-2}$ \\ \hline 
Pure error & $\text{SSPE} = \sum \sum (Y_{ij} - \bar{Y}_j)^2$ & $n-c$ & $\text{MSPE} = \frac{\text{SSPE}}{n-c}$ \\ \hline 
Total & $\text{SSTO} = \sum \sum (Y_{ij} - \bar{Y})^2$ & $n-1$ & \\ \hline \end{tabular} $$
\item It can be shown that the mean squares MSPE and MSLF have the following expectations when testing whether the regression function is linear $$ \begin{aligned} \expe{\text{MSPE}} &= \sigma^2 \\ \expe{\text{MSLF}} &= \sigma^2 + \frac{\sum n_j(\mu_j - (\beta_0 + \beta_1X_j))^2}{c-2} \end{aligned} $$ 
\item The terminology ``error sum of squares" and ``error mean square" is not precise when the regression function under test in $H_0$ is not the true function since the error sum of squares and error mean square then reflect the effects of both the lack of fit and the variability of the error terms 
\item Note that when concluding $H_0$, or that $\beta_1 = 0$, it is to say that there is no linear association between $X$ and $Y$, not ``no relation" between the two variables 
\item The general linear test approach can be used to test the appropriateness of other regression functions; only the degrees of freedom for SSLF needs be be modified; in general, $c-p$ degrees of freedom are associated with SSLF, where $p$ is the number of parameters in the regression function 
\item The alternative $H_A$ in the test hypotheses includes all regression functions other a linear one
\item When concluding that the employed model in $H_0$ is appropriate, the usual practice is to use the error mean square MSE as an estimator of $\sigma^2$ in preference to the pure error mean square MSPE, since the former contains more degrees of freedom
\item Observations at the same level of $X$ are genuine repeats only if they involve independent trials with respect to the error term
\item When no replications are present in a data set, an approximate test of lack of fit can be conducted if there are some cases at adjacent $X$ levels for which the mean responses are quite close to each other; such adjacent cases are grouped together and treated as pseudoreplicates, and the test for lack of fit is then carried out using these groupings of adjacent cases
\end{itemize} 

\subsection{Overview of Remedial Measures}
\begin{itemize}
\item If the simple linear regression model is not appropriate for a data set, there are two basic choices: \begin{itemize} 
\item Abandon the regression model and develop and use a more appropriate model - may lead to more complex procedures for estimating the parameters 
\item Employ some transformation on the data so that the regression model is appropriate for the transformed data - can lead to relatively simple methods of estimation and may involve fewer parameters than a complex model \end{itemize} 
\item When the regression function is not linear, a direct approach is to modify the regression model by altering the nature of the regression function such as by using a quadratic or exponential regression function 
\item When the error variance is not constant but varies in a systematic fashion, a direct approach is to modify the model to allow this and use the method of weighed least squares to obtain the estimators of the parameters; transformations can also be effective in stabilizing the variance 
\item When the error terms are correlated, a direct remedial measure is to work with a model that calls for correlated error terms; a simple remedial transformation that is often helpful is to work with first differences 
\item Lack of normality and nonconstant error variance frequently go hand in hand; it is often the case that the same transformation that helps stabilize the variance is also helpful in approximately normalizing the error terms 
\item When residual analysis indicates that an important predictor variable has been omitted from the model, the solution is to modify the model to include that predictor variable 
\item When outlying observations are present, use of the least squares and maximum likelihood estimators for the regression model may lead to serious distortions in the estimated regression function; when the outlying observations should not be discarded, it may be desirable to use an estimation procedure that places less emphasis on such outlying observations
\end{itemize} 

\subsection{Transformations}
\begin{itemize} 
\item When the distribution of the error terms is reasonably close to a normal distribution and the error terms have approximately constant variance, transformations on $X$ should attempted because a transformation on $Y$  may materially change the shape of the distribution of the error terms from the normal distribution and may also lead to substantially differing error term variances 
\item When the error variance is constant and the regression pattern is \begin{itemize} 
\item increasing and concave down, consider $X' = \log_{10} X$ or $X' = \sqrt{X}$
\item increasing and concave up, consider $X' = X^2$ or $X' = \exp(X)$ 
\item decreasing, consider $X' = 1/X$ or $X' = \exp(-X)$ \end{itemize} 
\item If some of the $X$ data are near zero and the reciprocal transformation is desired, the origin can be shifted by using the transformation $X' = 1/(X+k)$ where $k$ is an appropriately chosen constant 
\item Scatter plots and residuals plots based on each transformation can be prepared and analyzed to decide which transformation is most effective 
\item Unequal error variances and nonnormality of the error terms frequently appear together and to remedy these departures from the simple linear regression model, a transformation on $Y$ is needed since the shapes and spreads of the distributions of $Y$ need to be changed, and can also help to linearize a curvilinear regression relation 
\item When the error variance is nonconstant and its pattern is \begin{itemize} 
\item increasing and concave down, consider $Y' = \sqrt{Y}$
\item decreasing and concave up, consider $Y' = \log_{10} Y$
\item increasing at constant slope, consider $Y' = 1/Y$ \end{itemize} 
\item When $Y$ may be negative, it may be desirable to introduce a constant into a transformation of $Y$; in the case of the logarithmic transformation, shift the origin in $Y$ and make all observations positive by using $Y' = \log_{10}(Y + k)$ where $k$ is an appropriately chosen transformation
\item Several alternative transformations of $Y$ may be tried, as well as some simultaneous transformations on $X$; scatter plots and residual plots should be prepared to determine the most effective transformation(s) 
\item When unequal error variances are present but the regression relation is linear, a transformation on $Y$ may not be sufficient since it may be change the linear linear relationship to a curvilinear and thus a transformation on $X$ may also be required; weighted least squares is another option in this case
\item The Box-Cox procedure can automatically identify a transformation from the family of power transformations on $Y$ which is most appropriate for correcting skewness of the distributions of error terms, unequal error variances and nonlinearity of the regression function 
\item The family of power transformations is of the form: $$ Y' = Y^\lambda $$ where $\lambda$ is a parameter to be determined from the data
\item The family of power transformations encompasses the following simple transformations 
$$ \begin{tabular}{|c|c|c|c|c|} \hline 
$\lambda = 2$ & $\lambda = .5$ & $\lambda = 0$ & $\lambda = -.5$ & $\lambda = -1.0$ \\ \hline 
$Y' = Y^2$ & $Y' = \sqrt{Y}$ & $Y' = \log_e(Y)$ & $Y' = \frac{1}{\sqrt{Y}}$ & $Y' = \frac{1}{Y}$ \\ \hline \end{tabular} $$ 
\item The normal error regression model with the response variable a member of the family transformations becomes $$ Y_i^\lambda = \beta_0 + \beta_1X_i + \eps_i $$ 
\item The Box-Cox procedure uses the method of maximum likelihood to estimate $\lambda$ as well as the other parameters $\beta_0$, $\beta_1$ and $\sigma^2$; thus the Box-Cox procedure identifies $\hat{\lambda}$, the maximum likelihood estimate of $\lambda$ to use in the power transformation 
\item A simple procedure for obtaining $\hat{\lambda}$ involves a numerical search in a range of potential $\lambda$ values; for each $\lambda$ value, the $Y_i^\lambda$ observations are first standardized so that the magnitude of the error sum of squares does not depend on the value of $\lambda$ $$ W_i = \begin{cases} K_i(Y_i^\lambda - 1) ~~~ &\lambda \neq 0 \\ K_2(\log_e(Y_i)) ~~~ &\lambda = 0 \end{cases} $$ where $$ \begin{aligned} K_2 &= \left( \prod_{i=1}^n Y_i \right)^{\frac{1}{n}} \\ K_1 &= \frac{1}{\lambda K_2^{\lambda-1}} \end{aligned} $$ 
Note that $K_2$ is the geometric mean of the $Y_i$ observations
\item Once the standardized observations $W_i$ have been obtained for a given $\lambda$ value, they are regressed on the predictor variable $X$ and the error sum of squares SSE is obtained; the maximum likelihood estimate $\hat{\lambda}$ is that value of $\lambda$ for which SSE is a minimum 
\item If desired, a finer search can be conducted in the neighborhood of the $\lambda$ value that minimizes SSE but the Box-Cox procedure is only used to provide a guide for selecting a transformation, so overly precise results are not needed 
\item At times, theoretical or a priori considerations can be utilized to help in choosing an appropriate transformation 
\item After a transformation has been tentatively selected, residual plots and other analyses described earlier need to be employed to ascertain that the simple linear regression model is appropriate for the transformed data
\item When transformed models are employed, the estimators $\beta_0$ and $\beta_1$ obtained by least squares have the least squares properties with respect to the transformed observations, not the original ones
\item The maximum likelihood estimate of $\lambda$ with the Box-Cox procedure is subject to sampling variability; in addition, the error sum of squares SSE is often fairly stable in a neighborhood around the estimate and thus it is reasonable to use a nearby $\lambda$ value for which the power transformation is easy to understand 
\item When the Box-Cox procedure leads to a $\lambda$ value near $1$, no transformation of $Y$ may be needed 
\end{itemize} 

\subsection{Exploration of Shape of Regression Function}
\begin{itemize}
\item Scatter plots often indicate readily the nature of the regression function; at other times, the scatter plot is complex and it becomes difficult to see the nature of the regression relationship, if any, from the plot
\item It is helpful to explore the nature of the regression relationship by fitting a smoothed curve, or nonparametric regression curve, without any constraints on the regression function; these are useful not only for exploring regression relationships but also for confirming the nature of the regression function when the scatter plot visually suggests the nature of the regression relationship
\item The method of moving averages uses the mean of the $Y$ observations for adjacent time periods to obtain smoothed values; special procedures are required for obtaining smoothed values at two ends of the series; the larger the successive neighborhoods used for obtaining the smoothed values, the smoother the curve will be 
\item The method of running medians is similar to the method of moving averages, except that the median is used the average measure in order to reduce the influence of outlying observations
\item With the method of running methods as well as with the moving average method, successive smoothing of the smoothed values and other refinements may be undertaken to provide a suitable smoothed curve
\item When the $X$ values are not equally spaced apart, a smoothing method such as band regression can be useful; this method divides the data set into a number of groups or ``bands" consisting of adjacent cases according to their $X$ levels; for each band, the median $X$ value and the median $Y$ value are calculated and the points defined by the pairs of these median values are then connected by straight lines
\item The lowess method, a more refined nonparametric method than band regression, obtains a smoothed curve by fitting successively linear regression functions in local neighborhoods 
\item The name lowess stands for ``locally weighted regression scatter plot smoothing" 
\item This method obtains the smoothed $Y$ value at a given $X$ by fitting a linear regression to the data in the neighborhood of the $X$ value and then using the fitted value at $X$ as the smoothed value 
\item Smoothed values at each end of the $X$ range are also obtained by the lowess procedure 
\item The lowess method uses a number of refinements in obtaining the final smoothed values to improve the smoothing and to make the procedure robust to outlying observations \begin{enumerate} 
\item The linear regression is weighted to give cases further from the middle $X$ level in each neighborhood smaller weights 
\item To make the procedure robust to outlying observations, the linear regression fitting is repeated, with the weights revised so that cases that had large residuals in the first fitting receive smaller weights in the second fitting
\item To improve the robustness of the procedure further, step 2 is repeated one or more times by revising the weights according to the size of the residuals in the latest fitting \end{enumerate}
\item To implement the lowess procedure, one must choose the size of the successive neighborhoods to be used when fitting each linear regression as well as the weight function that gives less weight to neighborhood cases with $X$ values far from each center $X$ level and another weight function that gives less weight to cases with large residuals; finally, the number of iterations to make the procedure robust must be chosen
\item In practice, two iterations appear to be sufficient to provide robustness
\item The weight functions suggested by Cleveland appear to be adequate for many circumstances; hence, the primary choice to be made for a particular application is the size of the successive neighborhoods
\item  The larger the size of the successive neighborhoods, the smoother the function but the greater the danger that the smoothing will lose essential features of the regression relationship
\item Smoothed curves are useful not only in the exploratory stages when a regression model is selected but they are also helpful in confirming the regression function chosen
\item To confirm: the smoothed curve is plotted together with the confidence band for the fitted regression function; if the smoothed curve falls within the confidence band, there is supporting evidence of the appropriateness of the fitted regression function
\item Note that smoothed curves, such as the lowess curve, do not provude an analytical expression for the functional form of the regression relationship; they only suggest the shape of the regression curve
\item The lowess procedure is not restricted to fitting linear regression functions in each neighborhood; higher-degree polynomials can also be utilized with this method 
\item Smoothed curves are also useful when examining residual plots to ascertain whether the residuals (or the absolute or squared residals) follow some relationship with $X$ or $\hat{Y}$
\end{itemize} 


\section{Simultaneous Inferences and Other Topics in Regression Analysis}
\subsection{Joint Estimation of $\beta_0$ and $\beta_1$}

\subsection{Simultaneous Estimation of Mean Responses}

\subsection{Simultaneous Prediction Intervals for New Observations}

\subsection{Regression through Origin}

\subsection{Effects of Measurement Errors}

\subsection{Inverse Predictions}

\subsection{Choice of $X$ Levels}


\section{Matrix Approach to Simple Linear Regression Analysis}
\subsection{Matrices}

\subsection{Matrix Addition and Subtraction}

\subsection{Matrix Multiplication}

\subsection{Special Types of Matrices}

\subsection{Linear Dependence and Rank of Matrix}

\subsection{Inverse of a Matrix}

\subsection{Some Basic Results for Matrices}

\subsection{Random Vectors and Matrices}

\subsection{Simple Linear Regression Model in Matrix Terms}

\subsection{Least Squares Estimation of Regression Parameters}

\subsection{Fitted Values and Residuals}

\subsection{Analysis of Variance Results}

\subsection{Inferences in Regression Analysis}

